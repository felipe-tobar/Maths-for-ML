%!TEX root = ../lecture_notes.tex


\section{Optimisation}
\label{cap:optimisation}

\textbf{NB:} in this chapter, we follow \cite{pml1Book}.\\

\noindent Optimisation is central to ML, since models are \emph{trained} by minimising a loss function (or optimising a reward function). In general, model design involves the definition of a training objective, that is, a function that denotes how good a model is. This training objective is a function of the training data and a model, the latter usually represented by its parameters. The best model is is the chosen by optimising this function. 

\begin{mdframed}[style=ejemplo, frametitle={\center Example: Linear regression (LR)}]

In the LR setting, we aim to determine the function
\begin{align}
  f \colon \R^M &\to \R\nonumber\\
  x &\mapsto f(x)=a^\top x + b,\quad a\in\R^M,b\in\R
   \label{eq:reg_lin_fn} 
\end{align}
conditional to a set of observations
\begin{equation}
	\datos=\{(x_i,y_i)\}_{i=1}^N\subset \R^M \times \R.
	\label{eq:training_set}
\end{equation}
 Using least squares, the function $f$ is chosen via minimisation of the sum of the square differences between observations $\{y_i\}_{i=1}^N$ and predictions $\{f(x_i)\}_{i=1}^N$. That is, we aim to minimise he loss:
\begin{equation}
	J(\datos,f) = \sum_{i=1}^N(y_i-f(x_i))^2 = \sum_{i=1}^N(y_i-a^
	\top x_i - b)^2.
	\label{eq:least_squares_cost}
\end{equation}
\felipe{Generate figure: Check fig 1 ML lecture notes}

\end{mdframed}


\begin{mdframed}[style=ejemplo, frametitle={\center Example: Logistic regression}]

Here, we aim to determine the function
\begin{align}
  f \colon \R^M &\to \R\nonumber\\
  x &\mapsto f(x)= \frac{1}{1 + e^{-\theta^\top x+b}},\quad \theta\in\R^M, b\in\R
   \label{eq:log_reg_fn} 
\end{align}
conditional to the  observations
\begin{equation}
	\datos=\{(x_i,c_i)\}_{i=1}^N\subset \R^M \times \{0,1\}.
	\label{eq:training_set_classif}
\end{equation}
 The standard loss function for the classification problem is the cross entropy, given by:
\begin{align}
	J(\datos,f) &= -\frac{1}{N} \sum_{i=1}^N \left(c_i\log f(x_i) + (1-c_i)\log(1-f(x_i))\right)\\
				&=  \frac{1}{N} \sum_{i=1}^N \left( \log(1+e^{-\theta^\top x+b}) -y_i(-\theta^\top x+b)  \right)
	\label{eq:x_entropy_cost}
\end{align}
\felipe{Generate figure}

\end{mdframed}



\begin{mdframed}[style=ejemplo, frametitle={\center Example: Clustering (K-means)}]

Given a set of observations
\begin{equation}
	\datos=\{x_i\}_{i=1}^N\subset \R^M,
	\label{eq:training_set_clustering}
\end{equation}
we aim to find cluster centres (or prototypes) $\mu_1,\mu_2,\ldots, \mu_K$ and \emph{assignment variables} $\{r_{ik}\}_{i,k=1}^{N,K}$, to minimise the following loss 
\begin{align}
	J(\datos,f) = \sum_{i=1}^N\sum_{k=1}^K r_{ik} ||x_i-\mu_k||^2\\
	\label{eq:clustering_cost} 
\end{align}
\felipe{Generate figure}

\end{mdframed}

\subsection{Terminology} % (fold)
\label{sub:opt_terminology}

We denote an optimisation problem as follows: 

\begin{equation}
	\min_{x\in\cX} f(x) \text{~ s.t.~ } g_i(x)\leq 0,~  h_j(x) = 0,~ i=1,\ldots,I,~ j=1,\ldots,J.
\end{equation}
We describe the components of this statement in detail: 

\begin{itemize}
	\item \textbf{Objective function:} The function $f:\cX\to \R$ is the quantity to be minimised, with respect to $x$. 
	\item \textbf{Optimisation variable:} Minimising $f$ requires fining the value of $x$ such that $f(x)$ is minimum. This is also written as
	\begin{equation}
	x_\star = \argmin_{x\in\cX} f(x) \text{~ s.t.~ } g_i(x)\leq 0,~  h_j(x) = 0.
	\end{equation}
	\item \textbf{Restrictions:} These are denoted by the functions $g_i$ and $h_i$ above, which describe the requirements for the optimiser in the form of equalities and inequalities, respectively. 
	\item \textbf{Feasible region:}  This is the subset of the domain that complies with the restrictions, that is 
	\begin{equation}
		C = \{x\in\cX, \text{~ s.t.~ } g_i(x)\leq 0,~  h_j(x) = 0,~ i=1,\ldots,I,~ j=1,\ldots,J \}
	\end{equation}
	\item \textbf{Local / global optima.} Values for the optimisation variable that solve the optimisation problem wither locally or globally. More formally: 
	\begin{align}
		\text{$x_\star$ is a local optima} &\iff \exists \lambda>0 \text{~ s.t.~ }  x_\star = \argmin_{ x\in\cX \text{~ s.t.~ } ||x-x_\star||\leq \lambda } f(x). \\
		\text{$x_\star$ is a global optima} &\iff x_\star = \argmin_{ x\in\cX } f(x). 
	\end{align}
\end{itemize}

\begin{mdframed}[style=discusion, frametitle={\center Interplay between constrains and local/global optima}]
\felipe{generate figure, how different restrictions change the number and type of optima}

\end{mdframed}


\begin{mdframed}[style=ejemplo, frametitle={\center Example: XXX}]
\felipe{Show a few parametric functions and indicate their (closed-form) minima}

\end{mdframed}

% subsection terminology (end)

\subsection{Continuous unconstrained optimisation} % (fold)
\label{sec:continuous_optimisation}

We will ignore constrains in this section, and we will focus on problems of the form

\begin{equation}
	\theta \in \argmin_{\theta\in\Theta} L(\theta).
\end{equation}
We emphasise that if $\theta_\star$ satisfies the above, then
\begin{equation}
	\forall \theta\in\Theta,~ L(\theta_\star) \leq L(\theta),
\end{equation}
meaning that it is a \textbf{global} optimum. However, as this might be very hard to find, we are also interested in local optima, that is, $\theta_\star$  such that 
\begin{equation}
\exists\, \delta > 0 \;\; \forall\, \theta \in \Theta \;\; 
\text{s.t.} \;\; 
\|\theta - \theta_\star\| < \delta 
\;\Rightarrow\;
L(\theta_\star) \le L(\theta).
\end{equation}

We now review the optimality conditions.

\begin{assumption}
The loss function $L$ is twice differentiable.
\end{assumption}

Denoting $g(\theta) = \nabla_\theta L(\theta)$ and $H(\theta) = \nabla_\theta^2L(\theta)$, we can state the following optimality conditions. 

\begin{itemize}
	\item \textbf{First order necessary condition:} If $\theta_\star$ is a local minimum, then 
	\begin{itemize}
		\item $\nabla_\theta L(\theta_\star)=0$
	\end{itemize}
	\item \textbf{Second order necessary condition:} If $\theta_\star$ is a local minimum, then 
	\begin{itemize}
		\item $\nabla_\theta L(\theta_\star)=0$
		\item $\nabla^ 2_\theta L(\theta_\star)$ is positive semidefinite
	\end{itemize}
	\item \textbf{Second order sufficient condition:} If $\theta_\star$ is a local minimum if and only if 
	\begin{itemize}
		\item $\nabla_\theta L(\theta_\star)=0$
		\item $\nabla^ 2_\theta L(\theta_\star)$ is positive definite
	\end{itemize}
\end{itemize}


\begin{mdframed}[style=ejemplo, frametitle={\center Example: different stationary points}]

Let us consider the function
\begin{align}
  f \colon \R^2 &\to \R\nonumber\\
  x &\mapsto f(x)=(p-1)x^2 + (p+1)y^2,\quad p\in\R
   \label{eq:example_fn_stationary_points} 
\end{align}
Observe that
\begin{equation}
	\nabla f = \begin{bmatrix}   2(p-1)x \\ 2(p+1)y   \end{bmatrix},
	\label{eq:nabla_example_fn_stationary_points}
\end{equation}
meaning that the only stationary points is  $(x,y) = (0,0)$. Furthermore, 
\begin{equation}
	\nabla^2 f = \begin{bmatrix}   2(p-1) & 0 \\ 0 & 2(p+1)   \end{bmatrix},
	\label{eq:nabla2_example_fn_stationary_points}
\end{equation}
where we have 3 possible cases: 

\begin{itemize}
	\item $p>1$: The stationary point is a minimum
	\item $-1<p<1$: The stationary point is a \emph{saddle point}
		\item $p<-1$: The stationary point is a maximum
\end{itemize}

\felipe{generate figure for all three cases, discuss case $|p|=1$}

\end{mdframed}


\subsection{Convex optimisation}
\label{subsec:convex_opt}


This setting is defined by having a convex objective function and a convex feasible region. Critically, in the setting of convex optimisation a local minimum (according to the first/second order conditions presented above) is a global minimum. We next formally provide the relevant definitions.

\begin{definition}[Convex set] $\cS$ is a convex set if $\forall x,x'\in\cS$, we have:
\begin{equation}
	\lambda x + (1 - \lambda)x' \in \mathcal{S},
	\quad \forall\, \lambda \in [0, 1].
\end{equation}
\end{definition}

\felipe{Generate figures for convex and non-convex sets}


\begin{definition}[Epigraph of a function] The epigraph of a function $f:\cX\to\R$ is the set defined by the region above the graph of the function, that is, 
\begin{equation}
	\operatorname{epi}(f) = \{\, (x, t) \in \cX \times \R \mid f(x) \le t \,\}.
\end{equation}


\begin{definition}[Convex function] $f$ is a convex function if its epigraph is convex. Equivalently, $f$ is convex is it is supported on a convex set and $\forall x,x'\in\cX$
\begin{equation}
	f\bigl(\lambda x + (1 - \lambda)x'\bigr)
	\;\leq\;
	\lambda f(x) + (1 - \lambda) f(x'),
	\quad \forall\, \lambda \in [0, 1].
\end{equation}
Furthermore, is the inequality is strict, we say that the function is \textbf{strictly convex}.
\end{definition}

\end{definition}


\begin{mdframed}[style=ejemplo, frametitle={\center Example: Convex functions (in 1D)}]

The following are convex function from $\R$ to $\R$:

\begin{itemize}
	\item $f(x) = x^2$
	\item $f(x) = e^{ax},\, a\in\R$
	\item $f(x) = -\log x$
	\item $f(x) = x^a,\, a>1,\, x>0$
	\item $f(x) = |x|^a,\, a\geq 1$
	\item $f(x) = x\log x,\, x>0$
\end{itemize}
\felipe{Generate figures, indicate epigraph (for convex and non-convex functions}

\end{mdframed}


We now review some important results in convex optimisation

\begin{proposition}
Consider $f:\cX\subset\R\to\R$ differentiable. We have that if $f'(x)\geq 0\, \forall x\in\R$, $f$ is non-decreasing
\end{proposition}
\begin{proof}
	By the fundamental theorem of calculus, we have that for $a,b\in\R, a<b,$
	\begin{equation}
		f(b)-f(a) = \int_a^b f'(x)dx,
	\end{equation}
	since $f'(x)\geq 0, \forall x\in[a,b]$, we have $\int_a^b f'(x)dx\geq 0$, therefore $f(b)\geq f(a)$, which means that $f$ is non-decreasing.
\end{proof}


\begin{proposition}
Consider $f:\cX\subset\R^d\to\R$ differentiable. The direction of maximum growth of $f$ at $x_0$ is along its gradient $\nabla f(x_0)$
\end{proposition}
\begin{proof}
	Let us consider $x' = x_0 + \rho u$, where $u\in\cX, ||u||=1$, and $\rho>0$ is a small constant. We find the maximum growth direction by maximising $f(x')-f(x_0)$ with respect to $u$. We consider the Taylor expansion
	\begin{equation}
		f(x') = f(x_0) + \nabla f(x_0)\rho u  + \mathcal{O}(\rho^2),
	\end{equation}
	and thus conclude that $f(x') - f(x_0) \simeq \nabla f(x_0)\rho u$, meaning that the maximum growth can be achieved by choosing $u$ parallel to $\nabla f(x_0)$. That is, $\nabla f(x_0)$ is the direction of maximum growth for $f$ at $x_0$. 
\end{proof}


\begin{theorem}
Suppose $f:\cX\subset\R^d\to\R$ twice differentiable, then $f$ is convex if and only if $\nabla^2$ is positive semi definite.
\end{theorem}
\begin{proof}
We consider $d=1$. Using the FTC, 
\begin{equation}
	f'(b) - f'(a) = \int_a^b f''(x)dx \geq 0,
 \end{equation}
 which implies that $f'$ is non-decreasing. Therefore (using FTC again),
 \begin{equation}
 	f(b) - f(a) = \int_a^b f'(x)dx \geq (b-a)f'(a),
 \end{equation}
 equivalently, 
\begin{equation}
	f(b)\geq f(a) ' (b-a)f'(a), \label{eq:tanget_PD1}
\end{equation}
meaning that the function $f$ \emph{is always above its tangent}. Evaluating \eqref{eq:tanget_PD1} for $(a,z)$ and $(b,z)$, where $z=(1-t)a + tb$, we have
\begin{align}
	f(z) &\geq f(a) + (z-a)f'(a)\\
	f(z) &\geq f(b) + (z-b)f'(b).
\end{align}
Then, multiplying the above equations by $(1-t)$ and $t$ respectively and summing them, we obtain: 
\begin{align}
	f(z) &\geq (1-t)f(a) + tf(b) + (1-t)(tb-ta)f'(a) +t[(1-t)a - (1-t)b]f'(b)\\
	     &=    (1-t)f(a) + tf(b) + (1-t)t(b-a) [f'(a) - f'(b)]\\
	     &\geq (1-t)f(a) + tf(b)
\end{align}
\end{proof}



\begin{mdframed}[style=ejemplo, frametitle={\center Example: Explore some functions}]

\felipe{Choose some functions, compute the derivative and Hessian, analyse them}

\end{mdframed}

\subsection{First order methods}
\label{sub:first_order_methods}

In general, finding a minimum by setting $\nabla f(x) = 0$ and solving for $x$ is not possible. For that reason, we will consider iterative methods based on gradients. 

The idea here is to go \emph{downhill} following the gradient towards the minimum (ignoring the curvature information for now). 

We will specify a starting point $x_0$ and calculate

\begin{equation}
	x_{t+1} = x_t + \eta_t d_t,
\end{equation}
where $\eta_t$ is a \emph{step size} and $d_t$ is a \emph{descent direction}, such as $-\nabla f$. Here, the subindex $\cdot_t$ represents the iteration number (starting from iteration $t=0$). We iterate until convergence, that is, until the elements in the sequence $x_t,x_{t+1}, x_{t+2},\ldots$ become constant (or very similar). If convergence is achieved, we will assume the minimum has been found. 

Note that there are several \emph{descent directions}, that is, directions $d_t$ such that 
\begin{equation}
	L( x_t + \eta_t d_t) \leq L(x_t).
\end{equation}
In fact, as long as $d_t^\top\nabla f \leq 0$, $d_t$ is a descent direction. Clearly, choosing $d_t = -\nabla f(x_t)$ is the \emph{steepest descent direction}.


\subsubsection{Role of the step size}
\label{ssub:stepsize}

The step size $\eta_t$ is also known as \emph{learning rate}. Furthermore,  we refer to the set $\{\eta_1,\eta_2,\ldots\}$ as the learning rate schedule.

We will usually consider a constant learning rate, that is, $\eta_t = \eta$, $\forall t\in\N$. Though this is the simplest choice, there are some concerns to this choice: if $\eta$ is too large, the iteration may fail to converge; whereas if it is too small, it may not converge at all. 


\begin{mdframed}[style=ejemplo, frametitle={\center Example: convergence for a parabola}]

\felipe{Plot the level sets of a parabola (or another convex function). Show how the steepest descent converges/diverges for different learning rates}

\end{mdframed}

The learning rate is usually tuned with heuristics. Since we will usually implement 
\begin{equation}
	x_{t+1} = x_t + \eta \nabla f(x_t),
\end{equation}
we will usually set $\nabla<||\nabla f||^{-1}$, as this will result in a stable autoregresive system for the sequence $x_t$. 



\subsubsection{Momentum}
\label{ssub:momentum}

In higher dimensions, we want to move faster in some directions and slow in other directions, depending on the value of the gradient in each coordinate. This can be achieved by: 
\begin{align}
	m_t &= \beta m_{t-1} + \nabla f(x_{t-1})\\
	x_t = x_{t-1} - \nabla_t m_t,
\end{align}
where $m_t$ is a smoothed version of the gradient, and $\beta\in[0,1]$ is a design (memory) parameter. This way, previous values of the gradient have effect on future updates: if a particular coordinate of the gradient is consistently large, then that coordinate will receive updated of a higher magnitude. This is particularly useful when the evaluation of the gradient is noisy. 

\subsubsection{Newton method}
\label{ssub:newton}
Newton's method is

\begin{equation}
	x_{t+1} = x_t -\eta_t H_t^{-1}\nabla f(x_t),	
\end{equation}
where recall that $H_t = \nabla^2 f(x_t)$ denots the Hessian of $f$ at $x_t$. This update follows from considering the second order approximation of the loss function arond the current point, that is: 

\begin{equation}
	L(x) \simeq L(x_t) + (\nabla f(x_t))^\top(x-x_t) + \frac{1}{2} (x-x_t)^\top H_t  (x-x_t),
\end{equation}
the minimum of which is given by 
\begin{equation}
	x_\star = x_t - H^{-1}_t\nabla L(x_t),
\end{equation}
where the learning rate can also be used. 

\begin{mdframed}[style=ejemplo, frametitle={\center Example: convergence for a parabola (2)}]

\felipe{Same a previous example, but with momentum and/or Newton}

\end{mdframed}



\subsection{Stochastic gradient descent}
\label{sub:SGD}


We now consider stochastic optimisation, where
\begin{equation}
	L(x) = \E_{q(z)}L(x,z),
\end{equation}
that is, when the loss function is random and we aim to minimise its expected value. 

For instance, in linear regression we have $L(\theta)=\E_{q(y|x)}(y-\theta^\top x)$.

In practice we are unable to compute this expectation since we don't know the law $q(z)$. However, since we usually have samples of $q$, we can do a sample approximation og the expectation. In fact, we will consider
\begin{equation}
	L(\theta) = \frac{1}{N}\sum_{i=1}^nL(x,z_i).
\end{equation}
In the linear regression example, this would be $L(\theta) = \frac{1}{N}\sum_{i=1}^n(y_i-\theta^\top x_i)^2.$ 

The gradient is then also approximated using a batch of, say, $B$ samples. That is, 

\begin{equation}
	\nabla L(\theta)\simeq \frac{1}{N}\sum_{i=1}^B\nabla L(x,z_i)
\end{equation}

\begin{mdframed}[style=ejemplo, frametitle={\center Example: random loss function for linear regression}]

\felipe{Plot a q function: its theoretial value and sample approximations using a finite set of datapoints}

\end{mdframed}



