%!TEX root = ../lecture_notes.tex


\section{Statistics}
\label{cap:stas}



\subsection{The statistical model}
\label{cap:stats_SM}


This part of the course focuses on \emph{mathematical statistics}, a methodological approach to inference based on probability, algebra, geometry, optimisation and measure theory. In this setup, we assume the availability of a dataset generated from a \emph{statistical model}, (aka, probabilistic or generative model) which is unknown. Here, our objective is to use the data to determine such models (and in particular its parameters) to ultimately learn the underlying properties of the data generating process and make predictions. A first step in this regard, is to define our \emph{statistical model}.

\begin{definition} [Statistical Model]
A statistical model is a set of probability distribution that can be considered as \emph{candidates} for the data-generating mechanism. 
\end{definition}

In some cases, the statistical model can be \emph{parametric}, that is, represented by a \textbf{finite} set of parameters. This includes the Normal distribution, parametrised by its mean and variance. In such cases, inference boils down to identifying the parameters. 

In this part of the course, we will consider a dataset $x$ belonging to an abstract space $\mathfrak{X}$, where typically $\mathfrak{X} = \mathbb{R}^n$. We will assume that $x$ is the realisation of an RV $X\in\mathfrak{X}$. We can understand the statistical model as the space of the possible hypotheses explaining the observed data. In this sense, a relevant question is: what is the law of $X$?.

In this course we will focus on parametric statistical models; this requires the rigorous definition of the parameters and their space. 

\begin{definition}[Parameters and parameter space] A parameter is a fixed but unknown quantity that specifies a feature of a random variable's distribution (e.g., its mean, variance, or proportion). Parameters will be denoted with the symbol $\theta\in\Omega$, where $\Omega$ is the set of all possible values for the parameter called \emph{parameter space}.
\end{definition}

We now denote the parametric family $\mathcal{P}$ as follows:
\[\mathcal{P} = \{\mathcal{P}_\theta | \theta \in \Omega \},\]
where $\mathcal{P}_\theta $ is a probability distribution \emph{indexed} by a parameter $\theta \in \Omega$. We will consider finite-dimensional $\Omega$, that is, $\Omega \subseteq \mathbb{R}^n$. Therefore, we denote:
\[\theta = [\theta_1, ..., \theta_n]^\top. \]

In summary:

\begin{itemize}
    \item $\theta$ is the parameter to be estimated from data
    \item $\Omega$ is the parameter space, where $\Omega \subseteq \mathbb{R}^n$
    \item $\mathcal{P}_\theta$ is probability over  $\mathfrak{X}$ (as a function of $\theta$)
    \item ${X}$ is a RV in $\mathfrak{X}$
    \item $x$ is the data, a realisatoin of $X$ and a a generic element of $\mathfrak{X}$.
\end{itemize}

\begin{mdframed}[style=ejemplo, frametitle={\center Computer manufacturer}]
A computer manufacturer wishes to estimate the lifetime of a particular component in its computers. To do this, data is first collected from computers that have been used under normal conditions. After consulting with experts, they decide to use a normal distribution to model the time it will take for the component of interest to fail. The useful life of the components is then modelled with an average lifetime $\theta$ and variance $\sigma^2$, with $\theta$ and $\sigma^{2}$  unknown parameters. If there are $N$ components, the random variables that model the useful life of each component will be identified as 
$X_1,\ldots,X_N$, with $X_i \sim \mathcal{N}(\theta,\sigma^{2}), \forall i=1,\ldots,N$. What do you think of this model?
\end{mdframed}

Statistical inference is a tool that allows us to solve a number of problems. The most relevant ones involve identification, that is, to discover the model that generated the data, and prediction, where we estimate a quantity that has not yet been observed. Of course, we seek to achieve both goals statistically, that is, by appropriately modelling the associated uncertainty.

\subsection{Statistics}


The initial setup in statistical inference features an observation dataset and our own assumptions. Therefore, the starting point in this regard is to apply transformations to the data; this underpins the construction of the so called \emph{statistics}. 


\begin{definition}[Statistic]
\label{def:estadístico}
Let $(\cT,\cA,\mu)$ be a probability space and $X\in\cX$ a RV with parametric distribution $\cP = \{P_\theta\ \tq\ \theta\in\Theta\}$. A statistic is a function of the realisation $X=x$ that is independent form the parameter $\theta$ (and the distribution $P_\theta$).
\begin{align}
\nonumber
	T:\ &\cX \rightarrow \cT\\
\nonumber
	&x\mapsto T(x).
\end{align} 

\end{definition}

\begin{remark}
It is critical to understand the difference between the value of the statistic $T(x)$ as a function of the data $x$ (which we consider to be the realization $X=x$ of the random variable) and the application of the function $T(\cdot)$ to the RV $X$, i.e., $T(X)$---which is also a RV. The former is a “fixed” value, while the latter is a random variable with its own probability distribution induced by $P_\theta$ and by the function $T$ (called the pushforward distribution $T_{\sharp P_\theta}$). 
\end{remark}


Some example statistics are: 
\begin{equation}
\nonumber
	T(x) = \frac{1}{n}\sum_{i=1}^nx_i,\qquad T'(x) = x, \qquad T''(x) = \min(x), \qquad T'''(x) = c\in \mathbb{R}.
\end{equation}

\paragraph{Suficiencia}
The objective of a statistic is to encapsulate or summarize the information contained in a sample $x = (x_1,x_2,\ldots,x_n)$ that is useful for determining parameters of the distribution of $X$ or some other property. Therefore, the identity function or the mean seem to fulfil this mission, at least intuitively. This is because, in practice, we want to extract as much information as possible from the data, which is achieved by the statistic $T$ (which summarizes all the data) and the statistic $T'$ (which contains all the data). On the contrary, note that the statistic $T''$ loses information, since only the minimum value of all the data obtained is extracted, thus losing the representation of, e.g., the dispersion of the sample. The same analysis can be made for the constant statistic, which contains no information from the data.

Informally, the idea of \emph{sufficiency} of a statistic wrt a parameter can be expressed as
\begin{displayquote}[Ronald Fisher, On the mathematical foundations of theoretical statistics] \it
“…no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter.”
\end{displayquote}

A formal definition is 
\begin{definition}[Sufficient statistic]
\label{def:estadístico_suficiente}
Let $(S,\cA,\mu)$ be a probability space and  $X\in\cX$ a RV with parametric distribution $\cP = \{P_\theta\ \tq\ \theta\in\Theta\}$. We say that the function $T:\cX\rightarrow\cT$ is a sufficient statistic for $\theta$ (or for $X$ or for $\cP$) if the conditional distribution $X|T(X)$ does not depend on the parameter  $\theta$, that is, 
\begin{align}
\nonumber
	P_\theta(X\in A | T(X)),\ A\in\cB(X), \text{does not depend on}\theta.
\end{align} 
\end{definition}

Let us consider the following examples. 

\begin{mdframed}[style=ejemplo, frametitle={\center Trivial sufficient statistic}]

	\label{ex:suficiencia_trivial}
	For any given parametric family $\cP$, the statistic defined by
	\begin{equation}
	\nonumber
		T(x) = x,
	\end{equation}
is a sufficient statistic. Indeed, $P_\theta(X\in A|X=x) = \ind_{A}(x)$ does not depend on the parameter $\theta$. 
\end{mdframed}

\begin{example}[Sufficient statistic for a Bernoulli RV]
	Let $x=(x_1,\ldots,x_n) \sim Ber(\theta)$, $\theta \in \Theta = [0,1]$, that is
	\begin{equation}
	\nonumber
		P_\theta(X=x) = \theta^{\sum x_i}(1-\theta)^{n-\sum x_i}.
	\end{equation}
	Observe that $T(x) = \sum\limits_{i=1}^{n} x_i$ is a sufficient statistic (by definition), since  
	\begin{alignat*}{3}
		P(X=x|T(X)=t) 	&= \frac{P(T(X)=t| X=x )P( X=x )}{P(T(X)=t)} \quad&&\text{(Bayes thm)}\\
						&= \frac{\ind_{T(x)=t}\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}{\binom{n}{t}\theta^t(1-\theta)^{n-t}} &&\text{(model, and sum of Bernoulli is  Binomial)}\\
						&= \ind_{T(x)=t} \binom{n}{t}^{-1} && \text{(since $T(x)=t$)}
	\end{alignat*}
	Therefore, $T(x)=\sum\limits_{i=1}^{n} x_i$ is a sufficient statistic.
\end{example}

In practice, sufficiency is no assessed by definition, but via a result known as Fisher-Neyman theorem. This will be left for personal study. 



\subsection{Estimators}

We now focus on a particular class of statistics that enables parameter estimation.

\begin{definition}[Estimator]
    Let $g:\Omega\rightarrow \mathbb{R}^n$, a function of the unknown parameter. An estimator of $g$ is a statistic $\hat{g}:\mathfrak{X}\rightarrow g(\Omega)$. We will say that $\hat{g}(x)$ is the estimation of $g(\theta)$. 
\end{definition}

\begin{remark}
    Estimators are particular instances of statistics: they are functions of the data with an image space that is equal to the image space of $\Omega$ through $g(\cdot)$.
\end{remark}

\begin{remark}
    Estimators can can be used to identify parameters, where $g(\theta)=\theta$, or other related relevant quantities. For instance, for the Gaussian model, where the parameter is given by $\theta = [\mu,\sigma^2]$, we might be interested in estimating the 95\% confidence interval, given by
    \begin{equation}
        g(\theta) = [\mu - 2\sigma,\mu + 2\sigma].
    \end{equation}
\end{remark}

\begin{example}[Estimator for the mean of a Gaussian RV]
	\label{ex:estimador_media}
	Let us consider  $X = (X_1,\ldots,X_n)\sim\cN(\mu,\sigma^2)$. An estimator of  $g(\theta) = g(\mu,\sigma) = \mu$ is the statistic
	\begin{equation}
	\nonumber
		\gh(X) = \frac{1}{n}\sum_{i=1}^nX_i.
	\end{equation} 
\end{example}



\subsection{Unbiased estimators} 
Estimators, being a deterministic function of the RV $X$, are themselves RVs. Therefore, we can assess an estimator in terms of its mean: we would ideally want that an estimator for $g(\theta)$ has a mean equal to that quantity.


\begin{definition}[Unbiased estimator]
	\label{def:estimador_insesgado}
	Let $\ghX$ be an estimator of $g(\theta)$. This is an unbiased estimator if
	\begin{equation}
	\nonumber
		\E{\ghX} = g(\theta),
	\end{equation}
	where the \emph{bias} of $\gh$ is defined as
	\begin{equation}
	\nonumber
		b_\gh(\theta) = \E{\ghX} - g(\theta).
	\end{equation}
	We say that the estimator is \textbf{asymptotically unbiased} if:
	\[\lim_n\E{\gh (X_1,...,X_n)} = g(\theta),\]
	meaning that, the estimator only becomes unbiased when an \emph{infinite amount of data} are available.
\end{definition}

\begin{remark}
Unbiased estimators are important in statistical theory and practice because they recover the true parameter on average. However, one should not focus on unbiasedness exclusively: performing well on average does not guarantee anything about the estimator's variability (variance) or about how large a sample is needed for the estimator to be reliable.
\end{remark}

The following examples illustrate the role of unbiased estimators in two parametric families. 

\begin{example}[Unbiased estimator for the Gaussian mean]
	\label{ex:estimador_in_media}
	The estimator of $g(\theta) =  \mu$ described in Example \ref{ex:estimador_media} is unbiased. Indeed: 
	\begin{equation}
	\nonumber
		\E{\ghX} = \E{\frac{1}{n}\sum_{i=1}^nX_i}	= \frac{1}{n}\sum_{i=1}^n\E{X_i}		= \frac{1}{n}\sum_{i=1}^n\mu = \mu.
	\end{equation}
\end{example}

The following example shows a \textbf{biased} estimator of the variance and how an \textbf{unbiased} estimator can be constructed based on it. 

\begin{example}[Pythagoras]
Let us consider a parametric family $\familiaparametrica$, and denote its mean and variance by $\mu$ an $\sigma^2$ respectively. Using observations $x_1,x_2,\ldots,x_n$, we can calculate the variance of the estimator of the mean, given by $\xb = \frac{1}{n}\sum_{i=1}^n x_i$ according to 
\begin{equation}
	\label{eq:varianza_media_muestral}
 	\Vt{\xb} = \Vt{\frac{1}{n}	\sum_{i=1}^n x_i}  \underbrace{=}_{\text{i.i.d.}}  \frac{1}{n^2}	\sum_{i=1}^n\Vt{ x_i} =\frac{\sigma^2}{n}.
 \end{equation} 
 This means that the estimator of the mean using $n$ observations has a variance $\sigma^2/n$.

 Let us now consider the following estimator for the variance: 
\begin{equation}
	\label{eq:est_varianza_sesgado}
	S_2 = \frac{1}{n}\sum_{i=1}^n (x_i-\xb)^2,
\end{equation}
and observe that the expected value of such estimator is
\begin{align}
	\label{eq:sesgo_varianza}
	\Et{S_2 } &= \Et{\frac{1}{n}\sum_{i=1}^n (x_i-\mu+\mu-\xb)^2}\nonumber\\
				&= \Et{ \frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2 + 2\frac{1}{n}\sum_{i=1}^n(x_i-\mu)(\mu-\xb) + \frac{1}{n}\sum_{i=1}^n(\mu-\xb)^2}\nonumber\\
				&= \Et{ \frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2 - 2(\mu-\xb)^2 + (\mu-\xb)^2}\nonumber\\
				&= \Et{ \frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2 - (\mu-\xb)^2}\nonumber\\
				&= \Vt{x_i} - \Vt{\xb}\quad\text{ver ecuación \eqref{eq:varianza_media_muestral}}\nonumber\\
				&= 	\sigma^2 + \sigma^2/n = \left(\frac{n+1}{n}\right)\sigma^2.
\end{align}
Therefore, the bias of the estimator in eq.~\eqref{eq:est_varianza_sesgado} is asymptotically unbiased. However, we can fix this bias by multiplying the estimator $S_2$ in eq.~\eqref{eq:est_varianza_sesgado}, by $n/(n+1)$. This results in a corrected estimator given by 
\begin{equation}
	\label{eq:est_varianza_insesgado}
	S'_2 = \frac{n}{n+1}S_2 =  \frac{1}{n+1}\sum_{i=1}^n (x_i-\xb)^2,
\end{equation}
for which it holds
\begin{equation}
	\Et{S'_2 } =  \left(\frac{n}{n+1}\right)\Et{S_2} \underbrace{=}_{\text{eq.}\eqref{eq:sesgo_varianza}} \left(\frac{n}{n+1}\right) \left(\frac{n+1}{n}\right)\sigma^2 = \sigma^2.
\end{equation}
As a consequence, the estimator $S'_2$ in eq.~\eqref{eq:est_varianza_insesgado} is unbiased.
\end{example}


\subsection{Loss functions}

We will now consider loss functions for parameters, that is, a function that determines the cost of estimating a parameter by a given estimator. \textbf{From no on, we will consider estimators of $g(\theta) = \theta$ for simplicity of notation.}

\begin{definition}[Loss function]
Let $\theta\in\Omega$ be a parameter and $a\in\Omega$ an estimator. The cost of estimating $\theta$ by $a$ is given by:
\begin{align}
    L: (\Omega \times \Omega) &\rightarrow \R\\
    (\theta \times a) &\mapsto L(\theta,a).
\end{align}

\end{definition}

\begin{example}[Quadratic loss function]
	\label{ex:costo_cuadrático}
A widely used loss function for comparing estimators is the \textbf{quadratic loss}, given by   
$$
L_2(\theta,a) = ||\theta-a||^{2}.
$$
\end{example}
\textbf{Discussion}: why do we use the exponent 2 and not other?


\begin{example}[$0-1$ loss]
	\label{ex:costo_0-1}
When estimating parameters in a space with no order relation, the $0-1$ loss is usually considered. This is given by
$$
L_{01}(\theta,a) = \ind_{\theta\neq a}.
$$
\end{example}

Since the estimator is a RV, so is the loss and we can calculate its expectation. This is known as \textit{risk}. 

\begin{definition}[Risk]
Given a parameter $\theta \in \Omega$, an estimator $a \in \Omega$, and a loss function $L(\theta,a)$, the \emph{risk} of estimating $\theta$ by $a$ is defined as the expected loss:
\begin{align}
    R: (\Omega \times \Omega) &\rightarrow \R,\\
    (\theta \times a) &\mapsto \mathbb{E}[\,L(\theta,a)\,].
\end{align}
\end{definition}

In particular, the risk associated to the quadratic loss in ex.~\ref{ex:costo_cuadrático} for an estimator $\phi$ of the parameter $\theta$, if given by: 
\begin{alignat}{3}
 	R(\theta, \phi)  &= \E{(\theta - \phi)^2}\nonumber\\
 						& = \E{\left(\theta - \bar{\phi}+ \bar{\phi} -\phi\right)^2}; \quad \text{denoting }\bar{\phi} = \E{ \phi}\nonumber\\
 						& = \E{(\theta - \bar{\phi})^2+2(\theta - \bar{\phi})\cancel{(\bar{\phi} -\phi)} +  (\bar{\phi} -\phi)^2}\nonumber\\
 						& = \underbrace{(\theta - \bar{\phi})^2}_{=b_{\phi}^2\ (\text{bias}^2)} +  \underbrace{\E{(\bar{\phi} -\phi)^2}}_{=V_{\phi}\ \text{(variance)}}.\label{eq:riesgo_cuad}
 \end{alignat} 
 From this result, we can see a justification for the use of the quadratic loss: its risk splits automatically in two terms expressing the accuracy (how unabiased it is) and precision (how disperse) of the estimator.


\subsection{Maximum likelihood estimator (MLE)} % (fold)
\label{sec:estimador_de_máxima_verosimilitud}

Since the parameter $\theta$ is unknown, calculating the loss related to an estimator $\hat\theta = \hat\theta(X)$ it not possible. To bypass the definition of a loss, we can construct an estimator based directly on: i) the pdf of $X\in\cX$, where the parameters $\theta$ appears explicitly, and ii) an observation dataset. To this end, the likelihood function defined as follows will be crucial

\begin{definition}[Likelihood function]
Let $X$ be a random variable with probability density (or mass) function 
$f_\theta(x)$ indexed by a parameter $\theta \in \Omega$. For an observed value 
$x$, the \emph{likelihood function} is defined as the mapping
\begin{align}
    \mathcal{L}: \Omega &\rightarrow \R_{\ge 0},\\
    \theta &\mapsto f_\theta(x),
\end{align}
which measures how plausible each parameter value $\theta$ is given the observation $x$.
\end{definition}


\begin{definition}[Maximum likelihood estimator (MLE)]
    Let $x$ be an observation (or realisation) of the random variable $X$, and let 
    $\mathcal{L}(\theta \mid x)$ be the corresponding likelihood function. The 
    \emph{maximum likelihood estimator} is defined by
    \begin{equation}
        \hat{\theta}_{\mathrm{MLE}} = \underset{\theta}{\arg\max}\, \mathcal{L}(\theta \mid x).
    \end{equation}
\end{definition}

\begin{remark}[Log-likelihood]
It is often convenient to work with the \emph{log-likelihood},
\[
    \ell(\theta) = \log \mathcal{L}(\theta),
\]
since the logarithm is a strictly increasing function and therefore preserves maximisers of the likelihood. Moreover, $\ell(\theta)$ is typically easier to manipulate and optimise, both analytically and computationally.
\end{remark}


\begin{remark}
Clearly, the MLE can be defined either in terms of the likelihood itself or any non-decreasing function of it, and it may also fail to exist or to be unique. In particular, we will focus on finding $\hat{\theta}_{\mathrm{MLE}}$ by maximising the log-likelihood $l(\theta) = \log L(\theta)$, which is usually easier to optimise computationally or analytically. In fact, we will often even ignore constants in the (log) likelihood, as these do not affect the maximiser of $L(\theta)$.
\end{remark}

\begin{example}[MLE: Bernoulli]
	\label{ex:bernoulli_MV}
	Let $X_1,\ldots X_n\sim\ber{\theta}$, the likelihood of $\theta$ is given by 
	\begin{equation}
		L(\theta) = \prod_{i=1}^n\theta^x_i(1-\theta)^{1-x_i},
	\end{equation}
	and its log-likelihood by $l(\theta) = (\sum_{i=1}^nx_i)\log \theta + (n-\sum_{i=1}^nx_i)\log(1-\theta)$. The MLE can be found by $\frac{\partial l(\theta)}{\partial \theta} = 0$:
	\begin{align*}
	\frac{\partial l(\theta)}{\partial \theta} =0 
	&\Rightarrow  (\sum_{i=1}^nx_i) \theta^{-1} = (n-\sum_{i=1}^nx_i)(1-\theta)^{-1}\\
	&\Rightarrow  \sum_{i=1}^nx_i (1-\theta) = (n-\sum_{i=1}^nx_i) \theta\\
	&\Rightarrow  \theta = \sum_{i=1}^nx_i/n.
	\end{align*}
\end{example}


\begin{exercise}
	Plot $l(\theta)$ in ex.~\ref{ex:bernoulli_MV}.
\end{exercise}

\begin{exercise}
	Find the MLE for $\theta = (\mu,\Sigma)$ for the RV $X\sim\cN(\mu,\Sigma)$.
\end{exercise}

\begin{example}
	Let $X\sim\uni{\theta}$, that is, $p(x) = \theta^{-1} \ind_{0\leq x \leq \theta}$. To calculate the likelihood function, observe that 
	\begin{equation}
		L(\theta) = \prod_{i=1}^n p_\theta(x_i),
	\end{equation}
	and note that $p_\theta(x_i) = 0$ si $x_i>\theta$. Therefore, $L(\theta)>0$ only if $\theta$ is greater than each observation, in particular, if $\theta\geq\max\{x_i\}_1^n$.

	Additionally, if we have $\theta\geq\max\{x_i\}_1^n$, then notice that $p_\theta(x_i) = 1/\theta$. As a consequence, the likelihood is given by 
		\begin{equation}
		L(\theta) = \theta^{-n}, \quad \theta\geq\max\{x_i\}_1^n
	\end{equation}
	and the MLE is $\thetaMV = \max\{x_i\}_1^n$.
\end{example}

The maximum likelihood estimator (MLE) satisfies several important theoretical properties:

\begin{itemize}
    \item \textbf{Consistency:} Under the assumption that the statistical model is \emph{identifiable}—i.e., different parameter values correspond to different probability distributions—the MLE converges to the true parameter as the number of observations grows. Intuitively, maximising the likelihood asymptotically minimises the Kullback--Leibler divergence between the true distribution and the distribution induced by a candidate parameter.
    
    \item \textbf{Equivariance:} If $\hat{\theta}_{\mathrm{MLE}}$ is the MLE of $\theta$, then for any transformation $g$, the MLE of $g(\theta)$ is $g(\hat{\theta}_{\mathrm{MLE}})$. This property allows us to compute MLEs under reparametrisations directly.
    
    \item \textbf{Asymptotic normality:} For large sample sizes, the MLE is approximately normally distributed around the true parameter with covariance matrix given by the inverse Fisher information. Formally, 
    \[
        \sqrt{n}(\hat{\theta}_{\mathrm{MLE}} - \theta) \ \xrightarrow{d} \ \mathcal{N}(0, I(\theta)^{-1}),
    \]
    where $I(\theta)$ is the Fisher information matrix.
    
    \item \textbf{Asymptotic efficiency:} As a consequence of asymptotic normality, the MLE achieves the Cramér--Rao lower bound for the variance in the limit of large $n$, making it asymptotically optimal among unbiased estimators.
\end{itemize}

In practice, these properties justify the widespread use of the MLE: it not only converges to the true parameter under mild assumptions, but also allows for straightforward reparametrisations and provides an estimator with minimal asymptotic variance.



\subsection{MLE in practice: three examples}
\label{sub:MV_tres_ejemplos}

\subsubsection{Gaussian linear regression} 
\label{sub:reg_lin}

Let us consider the task of modelling the number of passengers travelling each month in an airline. Experts have established that this variable as a quadratic growing trend and an oscillatory component of annual frequency. These phenomena can be explained by population growth, diminishing costs of operation, and the yearly seasonality of economic activity. 

Assuming that the number of passenger is a RV, we can model its (conditional) distribution wrt time $t$ by a normal distribution parametrised by 
\begin{equation}
	X \sim \cN\left(\theta_0 + \theta_1 t^2 + \theta_2\cos(2\pi t/12), \theta_3^2\right),
\end{equation}
where $\theta_0,\theta_1,\theta_2$ control the mean and $\theta_3$ the variance. 

Consequently, if our observations are given by $\{(t_i,x_i)\}_{i=1}^n$, we can write the log-likelihood of $\theta$ as 
\begin{align}
	\label{eq:logV_ejemplo_reg}
	l(\theta) 	&=\loga{\prod_{i=1}^n \frac{1}{\sqrt{2\pi\theta_3^2}}\expo{-\frac{(x_i-\theta_0 - \theta_1 t^2 - \theta_2\cos(2\pi t/12))^2}{2\theta_3^2}}}\nonumber \\
	&=\frac{n}{2}\loga{2\pi\theta_3^2}  - \frac{1}{2\theta_3^2}\sum_{i=1}^n (x_i - \theta_0 - \theta_1 t_i^2 - \theta_2\cos(2\pi t_i/12))^2
\end{align}
where we can see that $\thetaMV$ can be calculated explicitely and it is a function of  $\{(t_i,x_i)\}_{i=1}^n$, since eq.~\eqref{eq:logV_ejemplo_reg} is quadratic in $[\theta_0,\theta_1,	\theta_2]$.


% subsection estimador_de_mv_en_la_práctica_tres_ejemplos (end)

\subsubsection{Classification} 
\label{sub:clasif}

The reason why $\hat{\theta}_{\mathrm{MLE}}$ could be calculated explicitly is that the Gaussian model with a linearly parametrised mean leads to a quadratic log-likelihood, where the maximum is unique and can be written in closed form. However, in many situations, a linear Gaussian model is not appropriate.

One example of this is the problem of credit scoring, where, based on a set of \emph{features} describing a client, a bank officer must decide whether or not to grant the requested credit. To make this decision, the officer can review the bank’s database and identify clients who, in the past, either repaid or defaulted on their loans, in order to determine the profile of a \emph{payer} and a \emph{non-payer}. A new client can then be \emph{classified} as a payer or non-payer based on their similarity to these groups.

Formally, let us denote the client’s features by $t \in \R^N$, and assume that the client repays the loan with probability $\sigma(t)$ and defaults with probability $1 - \sigma(t)$, where the function $\sigma(t)$ is to be defined. This is equivalent to constructing the random variable $X$.
\begin{equation}
 	X|t \sim \ber{\sigma(t)}
 \end{equation} 
 where $X=1$ represent a paying client, and $X=0$ the opposite. A usual choice for the function $\sigma(\cdot)$ is the logistic function applied to a linear transformation of$t$, that is, 
 \begin{equation}
 	\Pr{(X=1|t)} = \frac{1}{1+e^{-(\theta_0 + \theta_1 t)	}}.
 \end{equation}
Observe that this is a linear classifier, where $\theta = [\theta_0, \theta_1]$ defines a hyperplane in $\R^N$ where clients $t\in\{t | 0\leq \theta_0 + \theta_1 t\}$ pay with probability greater or equal to 1/2, and the rest with probability less or equal to 1/2. This is known as \textbf{logistic regression}. 

Therefore, using the bank data $\{(x_i,t_i)\}_{i=1}^n$, what is the MLE for $\theta = [\theta_0, \theta_1]$? To answer this, notice that $\ell$ can be written as 
\begin{align*}
	l(\theta) &= \log \prod_{i=1}^n p(x_i|t) \\
			  &= \sum_{i=1}^n x_i \log \sigma(t) + \left(n-\sum_{i=1}^n x_i\right)\log(1-\sigma(t))\\
			  &= \sum_{i=1}^n x_i \log \frac{1}{1+e^{-(\theta_0 + \theta_1 t)	}} + \left(n-\sum_{i=1}^n x_i\right)\log(1-\frac{1}{1+e^{-(\theta_0 + \theta_1 t)	}})
\end{align*}
This expression does not have a global minima and, even though its gradient can be computed, we cannot solve $\partial l(\theta)/\partial \theta =0$ analytically. Finding the MLE thus requires gradient descent.  

\subsubsection{Latent variables: \textit{Expectation-Maximisation}} 
\label{sub:EM}

In certain scenarios, it is natural to assume that our data come from a mixture of models. For example, consider the distribution of heights in a population: we can naturally model this as a mixture of marginal distributions for male and female heights separately, i.e.,
\begin{align*} 
L(\theta) &= \prod_{i=1}^n \left( p\cN(X|\mu_H,\Sigma_H) + (1-p)\cN(X|\mu_M,\Sigma_M) \right)\\ 
&= \prod_{i=1}^n \left( p\frac{1}{\sqrt{2\pi\Sigma_H^{-1}}}\expo{\frac{-(x_i-\mu_H)^2}{2\Sigma^2_H}} + (1-p)\frac{1}{\sqrt{2\pi\Sigma_M^{-1}}}\expo{\frac{-(x_i-\mu_M)^2}{2\Sigma^2_M}}\right). 
\end{align*}

Optimising this expression with respect to all five components of $\theta$ is difficult, particularly because of the sum inside the product, which prevents simplification via the logarithm.

A key difference of this model compared to previous ones is the implicit introduction of a \emph{latent variable} that indicates which Gaussian generated each observation. If we knew this latent variable, the problem would become much simpler. Indeed, assume for a moment that we have access to observations ${z_i}{i=1}^n$ of the random variables ${Z_i}{i=1}^n$, where each $Z_i$ indicates which model generated $x_i$ (e.g., $Z_i = 0$ for male, $Z_i = 1$ for female).

In this case, let us consider the \textbf{complete data} ${(x_i, z_i)}{i=1}^n$ and write the complete-data log-likelihood as
\begin{align*} 
l(\theta|z_i,x_i) &= \log\prod_{i=1}^n \cN(X|\mu_H,\Sigma_H)^{z_i} \cN(X|\mu_M,\Sigma_M)^{(1-z_i)}\\ 
&\hspace{-3em}= \sum_{i=1}^n \left( z_i\log\frac{1}{\sqrt{2\pi\Sigma_H^{-1}}}\expo{\frac{-(x_i-\mu_H)^2}{2\Sigma^2_H}} + (1-z_i)\log\frac{1}{\sqrt{2\pi\Sigma_M^{-1}}}\expo{\frac{-(x_i-\mu_M)^2}{2\Sigma^2_M}}\right). \end{align*}
This objective function is much easier to optimise; however, it is not \emph{observed} because the random variable $Z$ is not available. One way to resolve this is to take the conditional expectation of the above expression with respect to $Z$, given the observed data and the current parameter estimates, then maximise this expectation with respect to $\theta$, and iterate. Specifically, since the expression is linear in $z_i$, it suffices to take its expectation:
\begin{align*} 
\Et{Z_i|\theta_t,x_i} &= 1\cdot\Prob{Z_i=1|\theta_t,x_i} + 0\cdot\Prob{Z_i=0|\theta_t,x_i}\\ 
&= \frac{\Prob{x_i|\theta_t,z_i=1} p(z_i=1)}{p(x_i|\theta)}\\ 
&= \frac{\Prob{x_i|\theta_t,z_i=1} p(z_i=1)}{p(x_i|z=1,\theta)p(z=1)+p(x_i|z=0 ,\theta)p(z=0)} \end{align*}



\subsection{Enfoque bayesiano}

En esta sección complementaremos el enfoque visto hasta ahora en cuanto a la incorporación de un modelo para la incertidumbre asociada al parámetro $\theta$. En el paradigma bayesiano, consideraremos que el parámetro es una variable aleatoria, es decir, $\Theta$, la cual para una realización particular tomar el valor $\Theta = \theta$. Esto nos permite información \emph{a priori} sobre la estimación a realizar, lo que permite, en muchos casos, ayudar a la inferencia. Una diferencia conceptual entre ambos enfoques, es que la estadística frecuentista evita la subjetividad, mientras que la estadística bayesiana se basa en la convicción del(a) investigador(a), para emitir juicios sobre una hipótesis.

En este capítulo, se estudiará la estadística bayesiana y se introducirán los mismos conceptos vistos anteriormente desde el punto de vista bayesiano.




\subsection{Contexto y definiciones principales}

\begin{definition}[Distribución a priori]
La información, sesgos y cualquier otra característica conocida de $\Theta$ codificadas mediante la propia ley de probabilidad de esta VA, la cual tiene densidad $p(\theta)$, nos referimos a esta como la \emph{densidad a priori} o simplemente \emph{prior}.
\end{definition}

Con esta definición, podemos ver que la densidad conjunta de las VAs $X,\Theta$ pueden ser expresadas combinando la densidad a priori con el modelo visto en las secciones anteriores, es decir, 
\begin{equation}
    p(x,\theta) = p(x|\theta)p(\theta)
    \label{eq:joint_bayes}
\end{equation}
donde hemos escrito $p(x|\theta)$ en vez de $p_\theta(x)$ para hacer explícito que ahora consideramos el parámetro como una variable aleatoria. 

Adicionalmente, con la distribución conjunta en la ecuación \eqref{eq:joint_bayes}, podemos definir:

\begin{definition}[Distribución marginal]
La distribución de $X$, obtenida mediante la desintegración de parámetro $\Theta$ del par $(X,\Theta)$, es decir 
\begin{equation}
    p(x) = \int_\Omega p(x|\theta)p(\theta)\d\theta
\end{equation}
es conocida como distribución marginal de $X$.
\end{definition}

Consideremos ahora que tenemos un conjunto de observaciones denotado por $\mathcal{D}$, de un modelo estadístico con parámetro $\Theta$, entonces podemos definir

\begin{definition}[Función de verosimilitud]
La densidad de probabilidad evaluada en un conjunto de observaciones $\cD$ como función del valor del parámetro $\Theta$, es decir 
\begin{align}
    L: \Omega &\rightarrow \R\\
    \theta&\mapsto l(\theta) = L_\cD(\theta) = p(\cD|\theta),
\end{align}
recibe el nombre de función de verosimilitud, o en inglés, \emph{likelihood}. 
\label{función_verosimilitud}
\end{definition}
\begin{remark}
La función de verosimilitud no es una densidad de probabilidad, es decir, no es cierto que
\begin{equation}
    \int_\Omega L(\theta)\d\theta = 1
\end{equation}
\end{remark}

\begin{remark}
Dado que la función función de verosimilitud usualmente adquiere una forma exponencial (como por ejemplo en el caso de la familia exponencial), hay ocasiones en donde es conveniente usar la \emph{log-verosmilitud}, esto es, 
\begin{equation}
l(\theta)=\log L(\theta)=\log p(\cD|\theta).
\end{equation}
Esta formulación será particularmente útil cuando queramos optimizar la verosimilitud. 
\end{remark}

\begin{remark}
En general (pero no siempre) asumimos observaciones $\cD = {X_1,\ldots,X_n}$, $X_i\sim p(x|\theta)$, que son i.i.d. En cuyo caso, la verosimilitud factoriza de la forma $L_\cD(\theta) = \prod_{i=1}^n L_{X_i}(\theta)$, con lo cual la log-verosimilitud toma la forma: 
\begin{equation}
l_\cD(\theta) = \sum_{i=1}^nl_{X_i}(\theta)
\end{equation}
\end{remark}

\begin{example}
Considere los datos $\cD = \{x_1,...,x_n\}$, donde $x_i$ es la observación de una VA $X_i\sim \mathcal{N}(\mu,\sigma^2)$ iid con $\sigma^2$ conocido. La función  de verosimilitud de $\mu$ está dada por:

\begin{align}
L(\mu)=p(\cD|\mu)&=\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}} 
\exp\left(\frac{-1}{2\sigma^{2}}(x_i-\mu)^{2}\right)\nonumber\\
&= \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right)^{n} \exp\left(\frac{-1}{2 \sigma^{2}} \sum_{i=1}^{n} (x_i - \mu )^{2}\right).
\end{align}

Luego, la log-verosimilitud está dada por:
\begin{equation}
    l(\mu)= \log L(\mu) = -\frac{n}{2} \log(2 \pi \sigma^{2}) + \frac{-1}{2\sigma^{2}} \sum_{i=1}^{n} (x_i - \mu )^{2}.
\end{equation}

\end{example}

Ahora estamos en condiciones de definir el elemento central de la inferencia bayesiana, sobre el cual todo el proceso de inferencia toma lugar. 

\begin{definition}[Distribución posterior]
\label{dist_posterior}
Dado el conjunto de observación $\cD$ la distribución \emph{posterior} del parámetro, es decir, considerando la inforamción reportada por los datos $\cD$, está dada por el teorema de Bayes mediante
\begin{align}
    p(\theta|\cD) = \frac{p(\cD)|\theta)p(\theta)}{p(\cD)}  \propto p(\mathcal{D}|\theta) p(\theta) \label{eq:posterior_def}
\end{align}

donde: 
\begin{itemize}
    \item $p(\theta)$ es el prior del parámetro.
    \item $p(\theta|\mathcal{D})$ es la posterior del parámetro. 
    \item $p(\mathcal{D}|\theta)$ es la verosimilitud
    \item $p(\mathcal{D}) = \int\Omega p(\mathcal{D}|\theta)p(\theta)\d \theta $ es la densidad marginal de los datos 
\end{itemize}
\end{definition}

La \emph{transición} de prior a posterior puede ser interpretada como el proceso de incorporar la evidencia de los datos (a través de la función de verosimilitud) para reducir la incertidumbre con respecto del valor del parámetro $\Theta$. De la ecuación \eqref{eq:posterior_def} podemos ver que este proceso, a veces referido como \emph{actualización bayesiana}, equivale a multiplicar por la verosimilitud, para luego normalizar, garantizando que $p(\theta|\cD)$ es en efecto una densidad de probabilidad. 

\begin{remark}
El símbolo $\propto$ en la ecuación \eqref{eq:posterior_def} es usado para indicar que el lado izquierdo es igual al lado derecho salvo una constante de proporcionalidad que depende de $\mathcal{D}$ y no de $\theta$. Con esto, cuando estemos calculando la posterior, solo nos enfocaremos en \emph{una versión proporcional}, pues luego la densidad posterior se puede encontrar mediante la normalización de esta última.
\end{remark}

\begin{example}[Posterior modelo Bernoulli]
\label{ej_post_bernoulli_1}
Sea $\theta$ la probabilidad de obtener cara al lanzar una moneda, y sean $X_1,..X_n$ $n$ resultados obtenidos al lanzar la moneda. Si no sabemos nada de $\theta$ antes del experimento, hace sentido tomar su prior como una distribución que de igual probabilidad a todo espacio de parámetros, es decir: $\theta \sim Unif(0,1)$. Notemos que el prior encapsula la infromación que tenemos antes del experimento. Modelamos $X_1,..X_n \sim Bernoulli(\theta)$. Entonces: 
$$
p(X_1,..X_n|\theta)=\prod_{i=1}^{n} \theta^{X_i} (1-\theta)^{1-X_i} = 
\theta^{\sum_{i=1}^{n} X_i} (1-\theta)^{n-\sum_{i=1}^{n}X_i}
$$
Notemos que en este caso, podemos calcular la distribución $p(X_1,..,X_n)$:
$$
p(X_1,..,X_n)=\int_{0}^{1} \theta^{\sum_{i=1}^{n} X_i} (1-\theta)^{n-\sum_{i=1}^{n}X_i} d\theta = B\left(\sum_{i=1}^{n} X_i +1 , n- \sum_{i=1}^{n} X_i +1 \right),
$$
donde $B(x,y)$ es la función beta:
$$
B(x,y)=\dfrac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}.
$$
Sea $s= \sum_{i=1}^{n}X_i$. Entonces la distribución a posteriori será:

$$
p(\theta|X_1,..X_n)= \dfrac{p(X_1,..X_n|\theta)}{p(X_1,..,X_n)} = \dfrac{1}{B(s+1,n-s+1)} \theta^{s} (1-\theta)^{n-s}.
$$

\end{example} 

Usualmente, es experimentos reales, los datos  $x_1,...,x_n$ son recibidos de forma secuencial, es decir, \emph{en línea}. De esta forma, es relevante notar que en primer lugar se observa $x_1$ primero, luego $x_2$, y así sucesivamente. \\

Consecuentemente, si se asume el prior para el parámetro $\theta$ dado por $p(\theta)$, es posible hacer la actualización bayesiana \emph{en línea} (o de forma adaptativa o continual), lo cual implica una corrección del modelo cada vez que se observan más datos. \\
Luego de observar $x_1$, la posterior $p(\theta|x_1)$ puede ser calculada como: 
$$
p(\theta|x_1) \propto p(x_1|\theta) p(\theta).
$$

Luego, al observar $x_2$, usamos el hecho que $X_1$ y $X_2$ son condicionalmente independientes dado $\theta$ y obtenemos: 
$$
p(\theta | x_1,x_2) \propto 
p(x_2|\theta) p(\theta|x_1) \propto p(x_1 |\theta) p(x_2|\theta) p(\theta) . 
$$
Con lo que para el caso general tenemos que 
$$
p(\theta | x_1,..x_n) \propto p(x_n|\theta) p(\theta|x_1,..x_{n-1}) \propto p(\theta) \prod_{i=1}^n p(\theta | x_i).
$$

\begin{remark}
Cuando las observaciones $\cD$ son condicionalmente independientes dado el parámetro $\theta$, entonces, la posterior $p(\theta|\cD)$ factoriza en las verosimilitudes de cada uno de los datos. 
\end{remark}

\begin{remark}
En la actualización bayesiana en línea, la posterior de la etapa $n$ sirve de prior de la etapa $n+1$.
\end{remark}


\subsection{Priors Conjugados}

La actualización bayesiana puede resultar en una posterior solo conocida de forma proporcional (cuando no es posible calcular la distribución marginal $p(x)$) o bien en una distribución que no pertenece a una familia conocida. Una herramienta que asegurar el cálculo de las distribuciones posteriores (incluyendo la constante de normalización) y que esta adopta una forma conocida es a través del uso de \textbf{priors conjugados}.
\begin{definition}

Sea un modelo con verosimilitud $p(x|\theta)$ y un prior sobre $\theta$ con densidad $p(\theta)$. Decimos que $p(\theta)$ es conjugado con la verosimiltud $p(x|\theta)$ si la posterior 
\begin{equation}
	p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}
\end{equation}
pertenece a la\textit{misma familia} que el prior $p(\theta)$. Donde pertenecer a la misma familia quiere decir que ambas tienen una densidad de probabilidad definida por la misma forma funcional, e.g., $f_\lambda(\theta)$ pero con distintos valores para el \textit{parámetro} $\lambda$, el cual es un \textit{hiperparámetro} del modelo.
\end{definition}


\begin{example}[continuación de Ejemplo \ref{ej_post_bernoulli_1}]
\label{ej_post_bernoulli_2}
Tarea: Verifique si el Ejemplo \ref{ej_post_bernoulli_1} es en efecto uno de prior conjugado. 
\end{example}

\begin{example}[Distribución Multinomial]
Consideremos una variable aleatoria multinomial $X\sim\mul{n,\theta}$ donde $\theta$ pertenece al simplex 
\begin{equation}
	\label{eq:simplex}
  \{\theta\in[0,1]^k:\theta_1 + \cdots + \theta_k = 1 \}.
 \end{equation} 
 La distribución multinomial genera vectores $X\in\N^k$ cuya $i-$ésima componente modela la cantidad de veces que ocurre el evento $i$ dentro de $k$ eventos en $n$ intentos. Por ejemplo, si lanzamos un dado balanceado 100 veces, el vector que contiene el conteo de veces que obtenemos cada cara puede modelarse como 
 \begin{equation}
  	\theta_\text{dado} \sim \mul{100,\left[\tfrac{1}{6},\tfrac{1}{6},\tfrac{1}{6},\tfrac{1}{6},\tfrac{1}{6},\tfrac{1}{6}\right]}.
  \end{equation} 
Denotando $X=[x_1,\ldots,x_n]$, observemos que una muestra multinomial $X\sim\mul{n,\theta}$ cumple con 
\begin{equation}
	\{x_i\}_{i=1}^k \subset \{0,1,\ldots,n\},\quad  \sum_{i=1}^kx_i = n.
\end{equation}

Finalmente, la distribución Multinomial está dada por 
\begin{equation}
 	\mul{X;n,\theta} = \frac{n!}{x_1!\cdots x_k!} \theta_1^{x_1}\cdots\theta_k^{x_k},
 \end{equation} 
 y es la generalización de las distribuciones: 
\begin{itemize}
	\item Bernoulli cuando $k=2$ y $n=1$; pues $\ber{X;\theta} = \theta^{x} (1-\theta)^{1-x}$
	\item Categórica (o \emph{multinoulli}): cuando $n=1$; pues $\cat{X;\theta} = \theta_1^{x_1}\cdots\theta_k^{x_k}$
	\item Binomial: cuando $k=2$; pues $\bin{X;n,\theta} = \binom{n}{x} \theta^{x}(1-\theta)^{n-x}$
\end{itemize}
\end{example}

Observemos que el parámetro $\theta$ en la distribución multinomial (y las otras tres) es precisamente una distribución de probabilidad (discreta). Es decir, el construir un prior $p(\theta)$ implica definir una distribución sobre distribuciones discretas.  


\begin{definition}[Distribución de Dirichlet]
Consideremos la  distribución de Dirichlet
\begin{equation}
	\theta \sim \dir{\theta|\alpha} = \frac{1}{B(\alpha)} \prod_{i=1}^k \theta_i^{\alpha_i-1},
\end{equation}
donde $\alpha = (\alpha_1,\ldots,\alpha_k)$ es el parámetro de concentración y la constante de normalización está dada por $B(\alpha)=\prod_{i=1}^k\Gamma(\alpha_i)/\Gamma(\sum_{i=1}^k\alpha_i)$. El soporte de esta distribución es el simplex presentado en la ecuación \eqref{eq:simplex}.
\end{definition}


En el caso $k=3$, la distribución de Dirichlet puede ser graficada en el simplex de 2 dimensiones. La Figura \ref{fig:dist_Dirichlet} presenta tres gráficos para distintos valores del parámetro de concentración. 

\begin{figure}[H]
\includegraphics[width=0.3\textwidth]{img/stats_dirichlet111.png}
\includegraphics[width=0.3\textwidth]{img/stats_dirichlet101010.png}
\includegraphics[width=0.3\textwidth]{img/stats_dirichlet1022.png}
\caption{Distribuciones Dirichlet para $k=3$ con parámetros de concentración $\alpha$ (desde izquierda a derecha) dado por $[1,1,1]$, $[10,10,10]$ y $[10,2,2]$. }.
\label{fig:dist_Dirichlet}
\centering
\end{figure}


Veamos a continuación que la distribución de Dirichlet es conjugada al modelo Multinomial, y consecuentemente para Bernoulli, Categórica y Binomial. En efecto, si $\theta \sim \dir{\theta;\alpha}$ y $X\sim\mul{X;n,\theta}$, entonces

\begin{align}
	p(\theta|x) &= \frac{\mul{x;n,\theta}\dir{\theta;\alpha}}{p(x)}\nonumber\\
				&=  \frac{n!}{ x_1!\cdots x_k!p(x) B(\alpha)} \prod_{i=1}^k \theta_i^{x_i + \alpha_i-1}\nonumber\\
				&=  \frac{1}{B(\alpha')} \prod_{i=1}^k \theta_i^{\alpha'_i-1}
				\label{eq:dirichlet_post}
\end{align}
donde $\alpha' = (\alpha'_1,\ldots,\alpha'_k) = (\alpha'_1 + x_1,\ldots,\alpha'_k+ x_k)$ es el nuevo parámetro de concentración.

\begin{example}
	Consideremos $\alpha = [1,2,3,4,5]$ y generemos una muestra de $\theta\sim\dir{\theta|\alpha}$. El siguiente código genera, grafica e imprime esta muestra. 
	\begin{lstlisting}[language=Python]
	import numpy as np
	alpha = np.array([1,2,3,4,5]) 
	theta = np.random.dirichlet(alpha)
	plt.bar(np.arange(5)+1, theta);
	print(f'theta = {theta}')
\end{lstlisting}
En nuestro caso, obtuvimos los parámetros $ \theta = [0.034, 0.171, 0.286, 0.185, 0.324]$.

 Ahora, usaremos un prior Dirichlet sobre $\theta$ con $\alpha_p = [1,1,1,1,1]$ para calcular la posterior de acuerdo a la ecuación \eqref{eq:dirichlet_post}. La Figura \ref{fig:post_Dirichlet} muestra 50 muestras de la distribución posterior para distintas cantidades de observaciones entre 0 y  $ 10^5$. 

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{img/stats_post_dirichlet_0.pdf}
\includegraphics[width=0.3\textwidth]{img/stats_post_dirichlet_10.pdf}
\includegraphics[width=0.3\textwidth]{img/stats_post_dirichlet_100.pdf}\\
\includegraphics[width=0.3\textwidth]{img/stats_post_dirichlet_1000.pdf}
\includegraphics[width=0.3\textwidth]{img/stats_post_dirichlet_10000.pdf}
\includegraphics[width=0.3\textwidth]{img/stats_post_dirichlet_100000.pdf}
\caption{Concentración de la distribución posterior en torno al parámetro real para un modelo $X\sim\mul{\theta}$ y una distribución a priori Dirichlet $\theta\sim\dir{\alpha}$. Se considera desde 0 hasta $10^5$ observaciones y cada gráfico (desde izquierda-arriba hasta derecha-abajo) muestra el parámetro real (linea roja quebrada), la media posterior (línea azul quebrada) y 50 muestras de la posterior (azul claro). Observe cómo la distribución a priori (línea azul quebrada en la primera figura) pierde importancia a medida que el número de observaciones aumenta.}
\label{fig:post_Dirichlet}
\end{figure}
\end{example}


\begin{example} \textbf{Modelo gaussiano ($\sigma^2$ conocido).} Consideremos el prior sobre la media $p(\mu) = \cN(\mu_0,\sigma_0^2)$, con lo que la posterior está dada por  
 \begin{align}
 	p(\mu|\mathcal{D}) &\propto \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\mu)^2\right) \frac{1}{\sqrt{2\pi\sigma_0^2}}\exp\left(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\right)\label{eq:post_normal_mu_1}\\
 	&\propto \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\right),\label{eq:post_normal_mu_2}
 \end{align} 
 donde la proporcionalidad viene de ignorar la constante $p(\mathcal{D})$ en la primera línea e ignorar todas las contantes que no dependen de $\mu$ en la segunda línea. Recordemos que estas constantes para $\mu$ incluyen a la varianza de $x$, $\sigma^2$, por lo que ignorar esta cantidad es solo posible debido a que estamos considerando el caso en que $\sigma^2$ es conocido. Completando la forma cuadrática para $\mu$ dentro de la exponencial en la ec.~\eqref{eq:post_normal_mu_2}, obtenemos
 \begin{equation}
 	p(\mu|\mathcal{D}) \propto \exp\left(-\frac{1}{2\sigma_n^2}(\mu - \mu_n)^2\right),\label{eq:post_normal_mu_3}
 \end{equation} 
 donde (ya definiremos $\mu_n$ y $\sigma_n^2$ en breve) como $p(\mu|\mathcal{D})$ debe integrar uno, la única densidad de probabilidad proporcional al lado derecho de la ecuación anterior es la Gaussiana de media $\mu_n$ y varianza $\sigma_n^2$. Es decir, la constante de proporcionalidad necesaria para la igualdad en la expresión anterior es
 \begin{equation}
     \int_\R\exp\left(-\frac{1}{2\sigma_n^2}(\mu - \mu_n)^2\right)\d\mu = (2\pi\sigma_n^2)^{n/2}.
 \end{equation} Consecuentemente, confirmamos que el prior elegido era efectivamente conjugado con la verosimilitud gaussiana, con lo que la posterior está dada por la siguiente densidad (gaussiana):
  \begin{equation}
 	p(\mu|\mathcal{D}) = \cN(\mu;\mu_n,\sigma_n^2) = \frac{1}{(2\pi\sigma_n^2)^{N/2}}\exp\left(-\frac{1}{2\sigma_n^2}(\mu - \mu_n)^2\right),\label{eq:post_normal_mu_4}
 \end{equation} 
 donde la media y la varianza están dadas respectivamente  por 
 \begin{align}
 	\mu_n &= \frac{1}{\tfrac{1}{\sigma_0^2} + \tfrac{n}{\sigma^2}} \left(\frac{1}{\sigma_0^2}\mu_0 + \frac{n}{\sigma^2}\bar{x} \right), \quad \text{donde } \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i\label{eq:post_Gm}\\
 	\sigma_n &= \left(\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}\right)^{-1}.\label{eq:post_Gv}
 \end{align}
\end{example}
\begin{remark}
	La actualización bayesiana transforma los parámetros del prior de  $\mu$ desde  $\mu_0$ y $\sigma_0^2$ hacia $\mu_n$ y $\sigma_n^2$ en las ecs.~\eqref{eq:post_Gm} y \eqref{eq:post_Gv} respectivamente. Notemos que los  parámetros de la posterior son combinaciones (interpretables por lo demás) entre los parámetros del prior y los datos, en efecto, la $\mu_n$ es el promedio ponderado entre  $\mu_0$ (que es nuestro candidato para $\mu$ antes de ver datos) con factor $\sigma_0^{-2}$ y el promedio de los datos $\bar{x}$ con factor $(\sigma^{2}/n)^{-1}$, que a su vez es el estimador de máxima verosimilitud. Es importante también notar que  estos  factores son las varianzas inversas---i.e., precisión---de $\mu_0$ y de $\bar{x}$. Finalmente, observemos que $\sigma_n$ es la \emph{suma paralela} de las varianzas, pues  si expresamos la ec.~\eqref{eq:post_Gv} en términos de \emph{precisiones}, vemos que la precisión inicial $\sigma_0^2$ aumenta un término $\sigma^2$ con cada dato que vemos; lo cual tiene sentido pues con más información es la precisión la que debe aumentar y no la incertidumbre (en este caso representada por la varianza).
\end{remark}
\begin{example} \textbf{Modelo gaussiano ($\mu$ conocido).} Ahora procedemos con el siguiente prior para la varianza, llamado Gamma-inverso:
 \begin{equation}
 	p(\sigma^2)= \text{inv-}\Gamma(\sigma^2;\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha) (\sigma^2)^{\alpha+1}}\exp(-\beta/\sigma^2)
 \end{equation}
 esta densidad recibe dicho nombre pues es equivalente a modelar la precisión, definida como el recíproco de la varianza $1/\sigma^2$, mediante la distribución Gamma. Los hiperparámetros $\alpha$ y $\beta$ son conocidos como parámetros de forma y de tasa (o precisión) respectivamente. 

 Con este prior, la posterior de la varianza toma la forma:
 \begin{align}
 	p(\sigma^2|\mathcal{D}) &\propto \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\mu)^2\right) \frac{\beta^\alpha}{\Gamma(\alpha) (\sigma^2)^{\alpha+1}}\exp(-\beta/\sigma^2)\\
 	&\propto  \frac{1}{(\sigma^2)^{N/2+\alpha+1}}\exp\left(-\frac{1}{\sigma^2}\left(\frac{1}{2}\sum_{i=1}^N(x_i-\mu)^2 +\beta\right) \right)\nonumber
 \end{align} 
 donde nuevamente la proporcionalidad ha sido mantenida debido a la remoción de las constantes. Esta última expresión es proporcional a una distribución Gamma inversa con hiperparámetros $\alpha$ y $\beta$ ajustados en base a los datos observados. 


\end{example}




Hay ocasiones en las que el conocimiento a priori sobre el parámetro no puede ser convenientemente expresado mediante una densidad de probabilidad pero sí una densidad que no necesariamente integra uno o incluso es (Lebesgue) integrable. Para reflejar esta idea, se usan priors impropios.

\begin{definition}[Prior impropia] Una distribución a priori impropia es una distribución que no es necesariamente de probabilidad (i.e., no integra 1), pero que de todas formas puede ser utilizada como distribución a priori en el contexto de inferencia bayesiana, pues la distribución posterior correspondiente si es una distribución de probabilidad apropiada. 
\end{definition}

\begin{remark} No es necesario usar la constante de normalización en las densidades a priori Gaussianas (o ninguna otra en realidad).
\end{remark}

\begin{remark} Veamos que un prior impropio puede incluso tener integral infinita, en el caso de la distribución normal $X\sim\cN(X;\mu,1)$,  $\mu\in\R$, podemos elegir $p(\mu)\propto1$ y escribir 
\begin{equation}
	p(\mu|x)\propto p(x|\mu)\cdot 1 = \cN(x;\mu,1) = \cN(\mu;x,1). 
\end{equation}
	
\end{remark}

Considerar distribuciones uniformes impropias como priors no informativas parece tener sentido, pues intuitivamente no estamos dando preferencia (mayor probabilidad a priori) a ningún valor del parámetro por sobre otro. Sin embargo, este procedimiento sufre de una desventaja conceptual.






\subsection{Estimadores bayesianos}

Si bien ya hemos estudiado el rol del prior en la inferencia bayesiana, hasta ahora no lo hemos considerado en la construcción de estimadores. En particular, el EMV no incorpora conocimiento a priori del parámetro. Con el objetivo de incorporar este conocimiento a priori en el cálculo de estimadores puntales, consideramos que en el caso general, podemos considerar otros estimadores puntuales a través de una función de pérdida asociada a estimar el parámetro $\theta$ mediante el estimador $\hat\theta$ dada por $L(\theta,\hat\theta)$. Con esto podemos definir los conceptos de riesgo y estimador bayesiano.

\begin{definition}[Riesgo bayesiano]
Para una función de pérdida $L(\theta,\hat\theta)$ y un conjunto de observaciones $\cD$, el riesgo bayesiano es la esperanza posterior de dicha función de pérdida, es decir
\begin{equation}
    R(\hat\theta) = \int_\Omega L(\theta,\hat\theta)p(\theta|\cD)\d\theta.
\end{equation}
\end{definition}



\begin{definition}[Estimador bayesiano]
Dado un conjunto de datos $\cD$ y un riesgo bayesiano $R(\theta)$, un estimador bayesiano es uno que minimiza el riesgo bayesiano:
\begin{equation}
    \theta_\text{Bayes}(\cD) = \arg\min_{\Omega} R(\theta).
\end{equation}
donde es implícito que $R(\cdot)$ se define con $\cD$.
\end{definition}

A continuación, se definirán estimadores bayesianos  con distintas funciones de costo o de riesgo.


\begin{definition}[Bayes' Least-Squares (BLS)]
El  caso estándar es la función de pérdida cuadrática $L_2(\theta,\hat\theta) = (\theta-\hat\theta)^2$ la cual resulta en el estimador dado por la media posterior $\theta_\text{Bayes}(\cD) = \E{\theta|\cD}$
\end{definition}


\begin{definition}[Minimum absolute-error (MAE)]

De forma similar,la función de costo $L_1(\theta,\hat\theta) = |\theta-\hat\theta|_1$ resulta en el estimador dado por la mediana posterior.
\end{definition}

Encontrar una función de pérdida para el máximo a posteriori es menos directo. Consideremos en primer lugar el caso $\theta\in\Omega$ discreto y la pérdida ``0-1''
\[   
L_\text{0-1}(\theta,\hat\theta) = 
     \begin{cases}
       0 &\quad\text{si } \theta = \hat\theta,\\
       1 &\quad\text{si no}. 
     \end{cases}
\]

El riesgo de Bayes asociado a $L_\text{0-1}(\theta,\hat\theta)$ (en el caso discreto) toma la forma
\begin{equation}
    R(\hat\theta) = \Prob{\theta\neq\hat\theta|\cD} = 1-\Prob{\theta = \hat\theta|\cD},
\end{equation}
lo cual es minimizado eligiendo $\hat\theta$ tal que $\Prob{\theta = \hat\theta|\cD}$ es máximo, es decir, el MAP. ¿por qué no es posible proceder de esta forma para el caso continuo? ¿cuál es la función de costo asociada al MAP en el caso continuo?

\begin{definition}[Estimador máximo a posteriori]
Sea $\theta \in \Theta$ un parámetro con distribución a posteriori $p(\theta |D)$ definida en todo $\Theta$. Entonces nos referiremos a su estimación puntual dada por: 
$$
\theta_{MAP}= \underset{\Theta}{\arg\max}\ p(\theta|D),
$$

como el estimador \emph{máximo a posteriori} (MAP). Se utiliza la siguiente función de costo:

\[C(a,b)=\begin{cases}
  1, &|a-b|>0\\
  0, \sim
\end{cases}\]
\end{definition}

\begin{remark}
Es posible encontrar el MAP solo teniendo acceso a una versión \emph{proporcional} a la distribución posterior, un escenario usual en inferencia bayesiana, o también mediante la maximización del logaritmo de ésta última. En efecto, 
$$
\theta_{MAP} = \underset{\theta \in \Theta}{\arg\max }\ p(\theta|\mathcal{D}) = \underset{\theta \in \Theta}{\arg\max }\ p(\mathcal{D}|\theta)p(\theta)= \underset{\theta \in \Theta}{\arg\max}\left(\underbrace{\log p(\mathcal{D}|\theta)}_{l(\theta)} + \log p(\theta)\right),
$$
donde hemos encontrado la maximización de  la función de log-verosimilitud, pero ahora junto al log-prior.
\end{remark}

\begin{remark}
Es relevante notar que el estimador MAP es una \emph{modificación} del EMV, pues ambos comparten una parte de la misma función objetivo (verosimilitud) con la diferencia que el MAP además incluye el término \emph{log-prior}. Esto puede entenderse como una regularización de la solución del problema de MV, en donde el término adicional puede representar las propiedades del estimador más allá de que las pueden ser exclusivamente revelada por los datos. 
\end{remark}

\begin{example}[Máximo a posterior para el modelo gaussiano]
En particular, para el modelo lineal y gaussiano que hemos considerado hasta ahora, podemos calcular $\theta_{MAP}$ para un prior Gaussiano de media cero y varianza $\sigma_\theta^2$. Éste está dado por (asumimos la varianza del ruido $\sigma_\epsilon^2$ conocida):	
\begin{align}
	\theta_\text{MAP}^\star 	&= \text{argmax } p(Y|\theta,X)p(\theta)\nonumber\\
	\text{[ind., def.]}\ &= \text{argmax } \prod_{i=1}^N \cN(y_i;\theta^\top x_i,\sigma_\epsilon^2)\cN(\theta;0,\sigma_\theta^2) \nonumber\\
	&= \text{argmax } \prod_{i=1}^N \frac{1}{\sqrt{2\pi}\sigma_\epsilon} \exp\left({\frac{-1}{2\sigma_\epsilon^2}(y_i-\theta^\top x_i)^2}\right)											\frac{1}{(\sqrt{2\pi}\sigma_\theta)^{M+1}} \exp\left({\frac{-||\theta||^2}{2\sigma_\theta^2}}\right) \nonumber\\
	 &= \text{argmax } \frac{1}{\sqrt{2\pi}\sigma_\epsilon} \frac{1}{(\sqrt{2\pi}\sigma_\theta)^{M+1}} \exp\left( \sum_{i=1}^N{\frac{-1}{2\sigma_\epsilon^2}(y_i-\theta^\top x_i)^2} -{\frac{||\theta||^2}{2\sigma_\theta^2}}\right) \nonumber\\
	\text{[log.]}\  &= \text{argmin } \sum_{i=1}^N{(y_i-\theta^\top x_i)^2} +{\frac{\sigma_\epsilon^2}{\sigma_\theta^2}||\theta |^2}.\nonumber 
	\label{eq:MAP_reg_lin}
\end{align}
Podemos ver que eligiendo un prior uniforme o de normal de varianza muy amplia, el MAP es equivalente al EMV. ¿qué significa esto? ¿qué comportamiento differente de EMV promueve el MAP en este caso?
\end{example}






\subsection{Posterior predictiva}

En la inferencia bayesiana las predicciones ocupan un rol relevante, pues luego de realizar inferencia sobre un modelo estadístico, en general estamos interesados estudiar cómo serán los siguientes datos genearados por el modelo. Para esto definiremos la predicción bayesiana de la forma

\begin{definition}[Posterior predictiva]
Para un conjunto de datos $\cD$ y un parámetro $\theta$, la densidad posterior predictiva está dada por
\begin{equation}
    p(x|\cD) = \int_\Omega p(x|\theta)p(\theta|\cD)\d\theta = \E{p(x|\theta) |\cD},
\end{equation}
es decir, el valor esperado del modelo estadístico con respecto a la ley posterior del parámetro (modelo).
\end{definition}
Podemos ahora considerar la posterior predictiva como nuestro modelo \emph{aprendido} y generar datos de él, donde nos encontramos frente al mismo dilema de un estimador puntual como en el caso anterior: es posible considerar muestras aleatorias, la media, la mediana o algún intervalo. 

\begin{remark}
La posterior predictiva es distinta (en general) a la predicción \emph{plug-in}, en donde consideramos en modelo estadístico $p_{\hat\theta}$ en base a un estimador (puntual) cualquiera $\hat\theta$. Desde esa perspectiva, la posterior predictiva equivale a considerar estimadores y modelos puntuales pero integrar todos ellos con respecto a la ley posterior. 
\end{remark}













