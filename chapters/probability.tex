%!TEX root = ../lecture_notes.tex


\section{Probability}
\label{cap:probability}

\subsection{Introduction}
\label{cap:prob_intro}

\textbf{NB:} in this chapter, we follow \cite{pml1Book}.\\

\noindent The field of probability studies, from a quantitative perspective, how \emph{likely} an event is. Conceptually, and perhaps historically, there are two main interpretations of probability. The first one is \textbf{frequentist probability}, which relates to frequency of occurrence, and then applies only to event that can be repeated an infinite number of times, such as throwing a dice or flopping a coin. As a consequence, this standpoint fails to assign a probability to events that are impossible to repeat, such as the average temperature of the Earth's surface reaching an all-time maximum in the year 2025. A second interpretation is that of \textbf{Bayesian probability}, which represents uncertainty about the occurrence of an event. This uncertainty might come from different sources, such unknown features in the experiments (epistemological uncertainty) or random components (aleatoric uncertainty). In this case, events need not be repeatable be be assigned with a probability.

A basic knowledge of probability theory, definitions and results in fundamental in ML. This is because in ML we design, train and deploy mathematical models that i) aim to capture/quantify uncertainty, and ii) deal with noise-corrupted training data. Therefore, a rigorous account of uncertainty is central to real-world ML applications.


\subsubsection{Definitions}
\label{cap:prob_defs}

To start studying probability, we will focus on the outcome $\omega$ of a hypothetical experiment, e.g., throwing a dice, where $\omega$ can take values $\{1,2,3,4,5,6\}$. In this context, we can define: 

\begin{definition}[Sample space] The set containing all the possible outcomes $\omega$ of an experiment is called sample space and is denoted by $\Omega$.
\end{definition}

\begin{definition}[Event space] The set $\cA$, referred to as event space, contains all possible subset of the sample space $\Omega$. Therefore, each element $A\in\cA$ represents a possible results of the experiment.
\end{definition}

\begin{definition}[Probability] The function
\begin{align}
 	 \Pb: \cA &\to [0,1]\\
 	 x &\mapsto \Pb(x)
 \end{align}
  denotes the probability of the result of the experiment falling inside the elements of $\cA$, that is, $\Pb(A) = \Pb(\omega\in A)$. The function $\Pb$ needs to fulfil some standard properties such as $\Pb(\Omega) = 1, \Pb(\emptyset) = 0,$ and $\Pb(A^c) = 1-\Pb(A)$.
\end{definition}

\begin{mdframed}[style=discusion, frametitle={\center $\cA$ vs $\Omega$}]

Why is the probability defined over $\cA$ and not over $\Omega$? Discuss via some examples

\end{mdframed}

\begin{mdframed}[style=ejemplo, frametitle={\center Examples}]
\felipe{Consider basic examples (dice, coin, uniform, rain), and present the sample space, event space, and probability}
\end{mdframed}

We refer to the triplet $(\Omega,\cA,\Pb)$ as \textbf{probability space}. 

\subsubsection{Basic properties}
\label{cap:prob_basicprop}

The joint probability of events $A$ and $B$ is denoted by
\begin{equation}
	\Pb(A\wedge B) =\Pb(A\cap B) = \Pb(A,B),
\end{equation}
note that if $\omega\in A$ and $\omega\in B$, then, $\omega\in A\cap B$.

The conditional probability of the event $A$ occurring, given that the event $B$ occurred is denoted by 
\begin{equation}
	\Pb(A|B) = \frac{\Pb(A,B)}{\Pb(B)},
\end{equation}
which is only valid when $\Pb(B)>0$.

Additionally, we say that events $A$ and $B$ are \textbf{independent} iff $\Pb(A,B)=\Pb(A)\Pb(B)$. Observe that this implies that 
\begin{equation}
	\Pb(A|B) = \frac{\Pb(A,B)}{\Pb(B)}= \frac{\Pb(A)\Pb(B)}{\Pb(B)} = \Pb(A),
\end{equation}
meaning that when $A$ and $B$ are independent, the latter provides no \textbf{information} for $A$.

Lastly, the probability of intersection, i.e., the probability of the events $\omega\in A$ or $\omega\in B$ is given by 
\begin{equation}
	\Pb(A \vee B) = \Pb(A) + \Pb(B) - \Pb(A \wedge B).
\end{equation}

\subsection{Random variables}
\label{cap:prob_RV}

In general, other than basic toy examples, we do not refer to the underlying experiment and its sample/event spaces. We instead consider \emph{quantities of interest} that result from the outcome of the experiment. Therefore, let us consider a map from the sample space to a target space $\cT$, e.g., $\cT=\R$, $\cT=\N$ or $\cT=\{1,2,3,\ldots,N\}$.


\begin{definition}[Random variable] A function 
\begin{align}
 	X: \Omega &\to \cT\\
 	\omega &\mapsto X(\omega)
 \end{align} 
is referred to as random variable. In general, we will denote the function as $X$ and its value as $x$, ignoring the explicit dependence on $\omega$.
\end{definition}

\begin{remark}
From now on, we will consider the outcome of the experiment (living in the sample space) as the value of the RV, this aims to have a more streamlined setup avoiding the need of a map from $\Omega$ to $\cT$. Accordingly, the event space $\cA$ and the probability $\Pb$ correspond to the outcomes (values) of $X$, we will usually denote the sample space by $\cX$. 
\end{remark}

\subsubsection{Discrete RVs}
\label{cap:prob_disc_RV}

If the sample space is finite or countably infinite, we will say that the RVis discrete. In this case, we denote the probability of the event $X=x$ by $\Pb(X=x)$. We define de following. 

\begin{definition}[Probability mass function (pmf)] 
\label{def:pmf}
For a discrete RV $X\in\cX$ the function 
\begin{align}
 	p_X: \cX &\to [0,1]\\
 	x &\mapsto p_X(x)=\Pb(X=x)
 \end{align} 
denotes the probability of the event in which the RV $X$ takes the value $x\in\cX$. Evidently, 
\begin{equation}
	\sum_{x\in\cX} p_X(x) = 1.
\end{equation}
\end{definition}

\begin{mdframed}[style=ejemplo, frametitle={\center Discrete RV}]
\felipe{Give an example, e.g., a dice, picking items and/or a infinite state space one}
\end{mdframed}

\subsubsection{Continuous RVs}
\label{cap:prob_cont_RV}

If $\cX = \R^d$, with $d>1$, or any other infinitely uncountable space, we say that the RV is continuous. Observe that in this case we cannot defined a pmf as in Def.~\ref{def:pmf}. This is because since there is an uncountable number of symbols in the sample space $\cX$, we cannot assign each of them with a probability strictly greater than zero and still a total probability mass equal to one. Therefore, we make use of the event space and assign probabilities to intervals rather tan particular values. 

\begin{definition}[Cumulative distribution function (cdf)] 
\label{def:cdf}
For a continuous RV $X\in\cX$, we can define the probability of $X$ to be less or equal that a given value $x\in\cX$ by 
\begin{align}
 	P_X: \cX &\to [0,1]\\
 	x &\mapsto P_X(x)=\Pb(X\leq x).
 \end{align} 
Observe that this also allows to denote the probability of $X$ lying in a bounded interval, that is, 
\begin{equation}
	\Pb(a\leq X \leq b) = P_X(b) - P_X(a).
\end{equation}
The cdf is monotonically non-decreasing by construction, and, when $\cX = \R$, we have 
\begin{align}
	\lim_{x\to -\inf} P_X(x) & = 0\\
	\lim_{x\to \inf} P_X(x) & = 1.
\end{align}
\end{definition}

The idea to assign probabilities to intervals, suggest that there exists a density of probability, that is, an amount of probability mass per unit of length. This is formalised via the following definition. 


\begin{definition}[Probability density function (pdf)] 
\label{def:pdf}
For a continuous RV $X\in\cX$, the probability density function of $X$ is given by the function 
\begin{align}
 	p_X: \cX &\to \R\\
 	x &\mapsto p_X(x)=\frac{d}{dx} P_X(x).
 \end{align} 
 This is possible when $P_X$ is differentiable. 
\end{definition}
Knowing the pdf, we can express: 
\begin{equation}
	\Pb(a\leq X \leq b) = \int_a^bp_X(x)dx = P_X(b) - P_X(a),
\end{equation}
which is a direct particular case of the fundamental theorem of Calculus. 

Also, for $\Delta x$ small, we have
\begin{equation}
	\Pb(x\leq X \leq x + \Delta x) = \int_x^{x+\Delta x}p_X(x)dx \simeq p(x)\Delta x.
\end{equation}

Lastly, if the cdf is \textbf{strictly monotonically increasing}, its inverse exists and it is known as the \textbf{quantile function}. The use quantiles is useful when assessing the value of the RV wrt the range of possible values. 



\begin{mdframed}[style=ejemplo, frametitle={\center Examples}]
\felipe{Show the Gaussian distribution and present cdf, pdf and quantile}
\end{mdframed}


\subsubsection{Properties and identities}
\label{cap:prob_properties}

Probability can be thought of as an extension of logical reasoning. The following properties are consistent with such extension. Let us first consider, in the same was as we did when we defined event probabilities, the following. 

Both for discrete (pmf) and discrete (pdf) RVs, we can denote $p(x,y)$ the joint pdf/pmf for RVs $X\in\cX$ and $Y\in\cY$. This can be seen as the probability of the \emph{intersection} event, where $X=x$ and $Y=y$. 

Furthermore, we can write 
\begin{align}
	p_X(x) &=  \sum_{y\in\cY}p(x,y)\quad \text{(discrete)}\\
	p_X(x) &=  \int_{\cY}p(x,y)dy\quad \text{(continuous)}.
\end{align}

In the ML community, this is known as \textbf{sum rule}, since it allows us to express the density $p(y)$ from a joint density by summing over all possible values.

A related expression is the so called \textbf{law of total probability}, which reads  

\begin{align}
		p_X(x) &=  \sum_{y\in\cY}p(x|y)p(y)\quad \text{(discrete)}\\
	p_X(x) &=  \int_{\cY}p(x|y)p(y)dy\quad \text{(continuous)}.
\end{align}
This operation is usually called \textbf{marginalisation}, or \textbf{disintegration} (of $y$).

The so called \textbf{product rule} is the decomposition of the joint law as follows
\begin{equation}
	p(x,y) = p(y|x)p(x),
\end{equation}
which allows to decompose the joint pmf/pdf using the law of the conditional event $X=x$ given that $Y=y$, and the marginal law of $Y$. A key consequence of the above expression is that, since the factorisation works in both ways, we have 
\begin{equation}
	p(x|y) = \frac{p(y|x)p(x)}{p(y)},
\end{equation}
which is known as the \textbf{Bayes theorem}. This is a fundamental result in probabilistic ML, since we usually observe measurements $y$ and want to infer parameters or latent variables $x$.

\begin{mdframed}[style=ejemplo, frametitle={\center Examples}]
\felipe{Let's go over the Month Python example or something about drawing cards}
\end{mdframed}

\subsubsection{Moments}
\label{cap:prob_moments}


\begin{definition}[Mean] 
\label{def:mean}
The mean, or expected value, of an RV $X\in\cX$, often denoted by $\mu = \mu_X$, is defined as
\begin{align}
	\E[X] &= \int_\cX xp_X(x)dx\quad\text{(continuous)}\\
	\E[X] &= \sum_{x\in\cX} xp_X(x)\quad\text{(discrete)}\\
\end{align}
 where in the continuous case we have assumed the existence of a pdf. 
\end{definition}
Observe that $\E[\cdot]$ is a linear operator, meaning that 
 \begin{align}
	\E[aX + b] &= a\E[X] + b\quad a,b\in\R\\
	\E[\sum_{i=1}^n X_i] &= \sum_{i=1}^n \E[ X_i], \\
\end{align}
where $X_i,X_2,\ldots, X_n$ is a collection of arbitrary RVs with finite mean. 


\begin{definition}[Variance] 
\label{def:variance}
The variance measures the \textbf{spread} of a distribution (with respect to its mean) and it given by
\begin{align}
	\V[X] &= \E[(X-\mu_X)^2]\\
				&= \int_\cX (x-\mu_X)^2p_X(x)dx\\
				&= \int_\cX (x^2 -2x\mu_X+\mu_X^2)^2p_X(x)dx\\
				&= \E[X^2] -\mu_X^2,
\end{align}
 where in the discrete case is obtained similarly. 
\end{definition}
A related quantity is the \textbf{standard deviation}, given by 
\begin{equation}
 	\text{std}[X] = \sqrt{\V[X]}.
 \end{equation} 
Let us consider an arbitrary sequence of \textbf{independent} RVs $X_i,X_2,\ldots, X_n$ and $a,b\in\R$.  Two key properties of the variance are 
\begin{align}
	\V[aX_1 + b] &= a^2\V[X_1]\\
	\V[\sum_{i=1}^n X_i]	&= \sum_{i=1}^n\V[ X_i]
\end{align}
MISSING: SOME INEQUALITIES AND CORRELATION

\subsection{Some particular RVs}
\label{cap:prob_someRVs}

\subsubsection{Bernoulli and binomial}
\label{cap:prob_bernoulli}

Consider tossing a coin, where $\Pb(\text{heads})=\theta$, where $0\leq \theta \leq 1$. Note that, as a consequence, $\Pb(\text{tails})=1-\theta$. This is called the Bernoulli distribution, and it is written as 
\begin{equation}
	Y\sim\ber(\theta).
\end{equation}
Bernoulli's pmf is given by 
\[
p_\ber(y) =
\begin{cases}
\theta, & y = 1, \\
1 - \theta, & y = 0, \\
0, & \text{otherwise.}
\end{cases}
\]
More concisely, we can write the pmf as 
\begin{equation}
	p_\ber(y) = \theta^y(1-\theta)^{1-y}.
\end{equation}

Let us now consider $N$ Bernoulli trials, denoted $Y_i\sim \ber(\cdot|\theta), i=1,\ldots,N$ -- this can be thought of as tossing a coin $N$ times. Define the RV $S$ as the total number of heads, that is
\begin{equation}
 	S = \sum_{i=1}^N I(y_i = 1).
 \end{equation} 
 The distribution of $S$ is 
 \begin{equation}
 	\bin(s|N,\theta) = \binom{N}{s}\theta^s(1-\theta)^{N-s},
 \end{equation}
which is known as the Binomial distribution, where $\binom{N}{s} = \frac{N!}{(N-s)!s!}$.


\begin{mdframed}[style=ejemplo, frametitle={\center Logistic regression}]

Within ML, the Bernoulli distribution is largely used for (probabilistic) classification. In such case we want to predict a binary variable $y\in\{0,1\}$, which might represent the fact that a sample is of class 1 (and not class 0), conditional to some observed features $x\in\R^d$. We can then model $p(y=1)$ as 
\begin{equation}
	p(y|x,\theta) = \ber(y|f(x,\theta)).
\end{equation}
Since $f(x,\theta)$ represents the parameter of $\ber(y)$, we need $0\leq f(x,\theta) \leq 1$. However, to avoid that requirement, we can leave $f(x,\theta)$ unconstrained and do 
\begin{equation}
	p(y|\theta,x) = \ber(y|\sigma(f(x,\theta))),
\end{equation}
where 
\begin{equation}
	\sigma(a) = \frac{1}{1+e^{-a}}
\end{equation}
is the logistic function. 
When we choose $f(x,\theta) = \theta_1^\top x + \theta_2$, we have the logistic regression model.

\felipe{Show plots of Log Reg and sigmoid function.}
\end{mdframed}

The extension of the Bernoulli (resp.~Binomial) distribution to the multivariate case, i.e., when the output of the experiments take values on categories $\{1,2,\ldots,K\}$ with $K>2$ is known as the categorical (resp.~Multinomial). These distributions will be left for personal study. 

\subsubsection{Uniform distribution}
\label{cap:prob_uniform}

MISSING

\subsubsection{Gaussian distribution}
\label{cap:prob_Gaussian}

The pdf the (scalar) Gaussian RV $X$ is given by 
\begin{equation}
	\cN(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2},
\end{equation}
when $\mu=0$ and $\sigma=1$, we say that $X$ is a standard normal. 

\begin{mdframed}[style=discusion, frametitle={Why Gaussian}]

The Gaussian distribution is widely used for modelling continuous RVs, this is, in part, because: 

\begin{itemize}
	\item Central limit theorem: The scaled sum of multiple independent RVs, with different distributions, converges to a Gaussian RVs
	\item The Gaussian distribution is the distribution with the maximum entropy (fewer structural assumptions) for a fixed variance
	\item Its parameters are easy to interpret
	\item The maths are simple when using it in applications
\end{itemize}

\end{mdframed}

The Gaussian distribution is central to model \textbf{observation noise}, and therefore is key in regression models. That is, one can consider a linear regression models where

\begin{equation}
	Y|x \sim \cN(y|a^\top x + b,\sigma^2),
\end{equation}
which is equivalent to 
\begin{equation}
	y = a^\top x + b + \epsilon,
\end{equation}
where $\epsilon \sim \cN(0,\sigma^2)$.

MISSING: MVN

\subsubsection{Other distributions}
\label{cap:prob_otherdist}

There is a number of relevant distributions that we will not cover due to time constraints. These are, among others, 

\begin{multicols}{3}
\begin{itemize}
    \item Student's $t$
    \item Cauchy
    \item Chi-square
    \item Laplace
    \item Beta
    \item inv-Gamma
    \item Gamma
    \item Exponential
    \item half Gaussian
\end{itemize}
\end{multicols}

\subsection{Transformations of RVs distributions}
\label{cap:prob_transRVs}

In ML, we are interested in the construction of expressive \textbf{generative models}, i.e., probability distributions. Despite having a large collection of known distributions, sometimes we need to design purpose-specific models; this is achieved by transforming a given RV. 

Consider $X\sim p_X(\cdot)$ and $Y=f(X)$ a deterministic transformation. We are interested in computing $p_Y(y)$.

The discrete case is straightforward, as we just need to sum over the possible (discrete) values. That is, 
\begin{equation}
	p_Y(y) = \sum_{x:y=f(x)}p_X(x).
\end{equation}

\begin{mdframed}[style=ejemplo, frametitle={\center Uniform into binary}]
Consider 
\begin{equation}
	p_X = Uniform(0,1,\ldots,9),
\end{equation}
and 
\[
y = f(x) =
\begin{cases}
1, & \text{if } x \text{ is odd}, \\[4pt]
0, & \text{if } x \text{ is even.}
\end{cases}
\]

\felipe{Plot the pmfs}
\end{mdframed}

The continuous case is a bit more involved, since we cannot directly sum over all possible values of the RV. However, we can work with the cdf to show that 
\begin{equation}
	P_Y(y) = \Pb(Y\leq y) = \Pb(f(x)\leq y) = \Pb(x\in\{x|f(x)\leq y\}).
\end{equation}
Let us notice that if $f$ is invertible, we can rely on the \textbf{change of variable theorem}, which states that
\begin{equation}
	\label{eq:CVT_scalar}
 	p_Y(y) = p_X(x)\left|\frac{dx}{dy}\right|.
 \end{equation} 
This identity can be proven using the cdf. When $f$ is invertible, we can denote $g=f^{-1}$ and note that (assuming that $f$ is non-decreasing)
\begin{equation}
	P_Y(y) = \Pb(Y\leq y) = \Pb(f(x)\leq y)= \Pb(x\leq g(y)) = P_X(g(y)).
\end{equation}
We can now take the derivative, to give: 
\begin{equation}
	p_Y(y) = \frac{d}{dy}P_X(g(y)) = \frac{dP_X(g(y))}{dx}\frac{dx}{dy}= p_X(g(y))\frac{dx}{dy}.
\end{equation}
If $f$ had been non-increasing, we would have obtained the same expression with the reversed sign. Therefore, we conclude eq.~\eqref{eq:CVT_scalar}.

In the multivariate case, the CVT reads
\begin{equation}
	\label{eq:CVT_vector}
 	p_Y(y) = p_X(x)\left|\text{det}J_q(y)\right|,
 \end{equation}
 where $J_q(y) = \frac{dg(y)}{dy}$ is the Jacobian of $g=f^{-1}$. 

  \begin{mdframed}[style=ejemplo, frametitle={\center Examples}]
\felipe{Give 1 or two examples with plots. They could be $\chi^2$, lognormal, uniform, half normal, etc.}
\end{mdframed}


\subsection{Monte Carlo sampling}
\label{cap:prob_mc}

Consider a set $\{x_1,x_2,\ldots,x_N\}$ of i.i.d. samples of an RV $X\sim p_X$. We are interested in approximating the law $p_X$, and also computing expectation wrt it. An empirical estimate of the law of $X$ is the so called \textbf{histogram}
\begin{equation}
	P_N(x) = \frac{1}{N} \sum_{i=1}^N \delta_{x_i}(x),
\end{equation}
where $\delta_{x_i}(\cdot)$ denotes de delta-Dirac mass in $x_i$. 

\begin{mdframed}[style=discusion, frametitle={\center Real approximation?}]

Does $P_N(x)$ becomes more and more similar to the true law of $X$ as $N$ grows? 

\end{mdframed}

Notice that $P_N$ allows to compute (approximate) expectations. Let us consider a function $f: x \mapsto f(x)$, and 
\begin{equation}
	I_f \eqdef  \int f(x)p(x)dx \simeq f(x)P_N(x) = \frac{1}{N} \sum_{i=1}^{N} f(x_i) \eqdef I_N. 
\end{equation}
Due to the Strong Law of Large Numbers (SLLN), we know that $I_N$ get arbitrarily close to $I_f$ as $N\to\infty$.

A relevant question is how to generate the samples needed to approximate $P_N$, this is known as \textbf{sampling}. There are three main approaches to sample from distributions, namely: 
\begin{itemize}
	\item Rejection sampling
	\item Importance sampling
	\item Markov chain Monte Carlo.
\end{itemize}
We will briefly revise them in class.












